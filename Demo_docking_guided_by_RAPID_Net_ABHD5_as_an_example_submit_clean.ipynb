{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi1n+O7ILTY1R887w6CORb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalytskyiJaroslaw/RAPID-Net/blob/main/Demo_docking_guided_by_RAPID_Net_ABHD5_as_an_example_submit_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oWLOWCbGDvLZ",
        "outputId": "00270fcc-b3f5-4c3d-a7eb-c5a213cf28e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Installing Conda environment in Colab... Please wait.\n",
            "\n",
            "‚úÖ Conda installation verified!\n",
            "‚ú®üç∞‚ú® Everything looks OK!\n",
            "\n",
            "üîß Installing OpenBabel from Conda-Forge... Please wait.\n",
            "\n",
            "‚úÖ Final Conda check...\n",
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openbabel'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2799953744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenbabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenbabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpybel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openbabel'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# @title **üîß Set Up the Colab Environment** { display-mode: \"form\" }\n",
        "\n",
        "# @markdown **Press the ‚ñ∂Ô∏è Play button to set up the environment.**\n",
        "# @markdown - ‚ö†Ô∏è After execution, the runtime will be restarted.\n",
        "# @markdown - After restarting the session, press ‚ñ∂Ô∏è **Play** again.\n",
        "# @markdown - This will install **Conda** and required dependencies.\n",
        "\n",
        "import os\n",
        "import io\n",
        "\n",
        "print(\"\\nüîß Installing Conda environment in Colab... Please wait.\")\n",
        "\n",
        "# --- Suppress installation output ---\n",
        "from IPython.utils import io\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "\n",
        "print(\"\\n‚úÖ Conda installation verified!\")\n",
        "condacolab.check()\n",
        "print(\"\\nüîß Installing OpenBabel from Conda-Forge... Please wait.\")\n",
        "with io.capture_output() as captured:\n",
        "    !conda install -q -y -c conda-forge openbabel=3.1.1\n",
        "\n",
        "print(\"\\n‚úÖ Final Conda check...\")\n",
        "condacolab.check()\n",
        "\n",
        "import openbabel\n",
        "from openbabel import pybel\n",
        "\n",
        "print(\"\\nüéâ Setup Complete! ü¶Å You can now use Conda & OpenBabel in Colab. üöÄ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ü¶Å Initialize the RAPID-Net Model** { display-mode: \"form\" }\n",
        "# @markdown RAPID-Net model parameters for pocket detection.\n",
        "# @markdown - The **voxel classification threshold** is the minimum density `threshold_input` for a voxel to be considered part of a pocket. *(Default: 0.5)*\n",
        "# @markdown - In other words, the $2\\,√Ö\\times 2\\,√Ö \\times 2\\,√Ö$ voxel is added to the pocket if probability > 50% by default.\n",
        "# @markdown - **\"Minority-reported\"** pockets include voxels detected by at least `min_vote_minimal` models from an ensemble of 5 models. *(Default: 1)*\n",
        "# @markdown - **\"Majority-voted\"** pockets include voxels detected by at least `min_vote_majority` models from an ensemble. *(Default: 3)*\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "threshold_input = 0.5  # @param {type:\"number\"}\n",
        "min_vote_minimal = 1   # @param {type:\"integer\"}\n",
        "min_vote_majority = 3   # @param {type:\"integer\"}\n",
        "\n",
        "import contextlib\n",
        "\n",
        "print(\"üîÑ Installing dependencies (py3Dmol, Biopython, RDKit, Meeko, PyMOL)...\")\n",
        "!pip install meeko > /dev/null 2>&1\n",
        "!pip install py3Dmol biopython rdkit==2024.09.1 > /dev/null 2>&1\n",
        "!conda install -c conda-forge pymol-open-source -y > /dev/null 2>&1\n",
        "print(\"‚úÖ Dependencies installed!\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, BatchNormalization, Activation, add, SpatialDropout3D, GlobalAveragePooling3D, Reshape, Dense, multiply, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_block(input_tensor, filters, stage, block):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    bn_axis = -1\n",
        "\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Conv3D(filters1, (1, 1, 1), kernel_initializer='he_normal', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv3D(filters2, (3, 3, 3), padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv3D(filters3, (1, 1, 1), kernel_initializer='he_normal', name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    x = add([x, input_tensor])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(input_tensor, filters, stage, block, strides=(2, 2, 2)):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    bn_axis = -1\n",
        "\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Conv3D(filters1, (1, 1, 1), strides=strides, kernel_initializer='he_normal', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv3D(filters2, (3, 3, 3), padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv3D(filters3, (1, 1, 1), kernel_initializer='he_normal', name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    shortcut = Conv3D(filters3, (1, 1, 1), strides=strides, kernel_initializer='he_normal', name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
        "\n",
        "    x = add([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def se_block(input_tensor, ratio=16):\n",
        "    channel_axis = -1\n",
        "    filters = input_tensor.shape[channel_axis]\n",
        "\n",
        "    se = GlobalAveragePooling3D()(input_tensor)\n",
        "    se = Reshape((1, 1, 1, filters))(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    x = multiply([input_tensor, se])\n",
        "    return x\n",
        "\n",
        "def RAPID_Net(input_shape=(36, 36, 36, 18), filters=18, dropout_rate=0.5, l2_lambda=1e-3):\n",
        "    params = {'kernel_size': 3, 'activation': 'relu', 'padding': 'same', 'kernel_regularizer': l2(l2_lambda)}\n",
        "\n",
        "    inputs = Input(shape=input_shape, name='input')\n",
        "\n",
        "    x = conv_block(inputs, [filters, filters, filters], stage=2, block='a', strides=(1, 1, 1))\n",
        "    x1 = identity_block(x, [filters, filters, filters], stage=2, block='b')\n",
        "\n",
        "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(x1)\n",
        "\n",
        "    x = conv_block(pool1, [filters*2, filters*2, filters*2], stage=4, block='a', strides=(1, 1, 1))\n",
        "    x2 = identity_block(x, [filters*2, filters*2, filters*2], stage=4, block='b')\n",
        "\n",
        "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(x2)\n",
        "\n",
        "    x = conv_block(pool2, [filters*4, filters*4, filters*4], stage=5, block='a', strides=(1, 1, 1))\n",
        "    x3 = identity_block(x, [filters*4, filters*4, filters*4], stage=5, block='b')\n",
        "\n",
        "    pool3 = MaxPooling3D(pool_size=(3, 3, 3))(x3)\n",
        "\n",
        "    x = conv_block(pool3, [filters*8, filters*8, filters*8], stage=6, block='a', strides=(1, 1, 1))\n",
        "    x4 = identity_block(x, [filters*8, filters*8, filters*8], stage=6, block='b')\n",
        "\n",
        "    pool4 = MaxPooling3D(pool_size=(3, 3, 3))(x4)\n",
        "\n",
        "    x = conv_block(pool4, [filters*16, filters*16, filters*16], stage=7, block='a', strides=(1, 1, 1))\n",
        "    x = identity_block(x, [filters*16, filters*16, filters*16], stage=7, block='b')\n",
        "\n",
        "    x = se_block(x)\n",
        "\n",
        "    up6 = concatenate([UpSampling3D(size=(3, 3, 3))(x), x4], axis=-1)\n",
        "    conv6 = Conv3D(filters=filters*8, **params)(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    conv6 = Conv3D(filters=filters*8, **params)(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling3D(size=(3, 3, 3))(conv6), x3], axis=-1)\n",
        "    conv7 = Conv3D(filters=filters*4, **params)(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    conv7 = Conv3D(filters=filters*4, **params)(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling3D(size=(2, 2, 2))(conv7), x2], axis=-1)\n",
        "    conv8 = Conv3D(filters=filters*2, **params)(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "\n",
        "    conv8 = Conv3D(filters=filters*2, **params)(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling3D(size=(2, 2, 2))(conv8), x1], axis=-1)\n",
        "    conv9 = Conv3D(filters=filters, **params)(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "\n",
        "    conv9 = Conv3D(filters=filters, **params)(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "\n",
        "    outputs = Conv3D(filters=1, kernel_size=1, kernel_regularizer=l2(1e-4), activation='relu', name='pocket')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='RAPID_Net')\n",
        "    return model\n",
        "\n",
        "\n",
        "def pocket_density_from_mol_RAPID_Net_run1(mol):\n",
        "    if not isinstance(mol, pybel.Molecule):\n",
        "        raise TypeError('mol should be a pybel.Molecule object, got %s '\n",
        "                        'instead' % type(mol))\n",
        "    if featurizer is None:\n",
        "        raise ValueError('featurizer must be set to make predistions for '\n",
        "                         'molecules')\n",
        "    if scale is None:\n",
        "        raise ValueError('scale must be set to make predistions')\n",
        "    prot_coords, prot_features = featurizer.get_features(mol)\n",
        "    centroid = prot_coords.mean(axis=0)\n",
        "    prot_coords -= centroid\n",
        "\n",
        "\n",
        "    resolution = 1. / scale\n",
        "    x = make_grid(prot_coords, prot_features,\n",
        "                              max_dist= max_dist,\n",
        "                              grid_resolution=resolution)\n",
        "    density = RAPID_Net_run1.predict(x)\n",
        "\n",
        "    origin = (centroid - max_dist)\n",
        "    step = np.array([1.0 / scale] * 3)\n",
        "\n",
        "    return density, origin, step\n",
        "\n",
        "def pocket_density_from_mol_RAPID_Net_run2(mol):\n",
        "    if not isinstance(mol, pybel.Molecule):\n",
        "        raise TypeError('mol should be a pybel.Molecule object, got %s '\n",
        "                        'instead' % type(mol))\n",
        "    if featurizer is None:\n",
        "        raise ValueError('featurizer must be set to make predistions for '\n",
        "                         'molecules')\n",
        "    if scale is None:\n",
        "        raise ValueError('scale must be set to make predistions')\n",
        "    prot_coords, prot_features = featurizer.get_features(mol)\n",
        "    centroid = prot_coords.mean(axis=0)\n",
        "    prot_coords -= centroid\n",
        "\n",
        "\n",
        "    resolution = 1. / scale\n",
        "    x = make_grid(prot_coords, prot_features,\n",
        "                              max_dist= max_dist,\n",
        "                              grid_resolution=resolution)\n",
        "    density = RAPID_Net_run2.predict(x)\n",
        "\n",
        "    origin = (centroid - max_dist)\n",
        "    step = np.array([1.0 / scale] * 3)\n",
        "\n",
        "    return density, origin, step\n",
        "\n",
        "def pocket_density_from_mol_RAPID_Net_run3(mol):\n",
        "    if not isinstance(mol, pybel.Molecule):\n",
        "        raise TypeError('mol should be a pybel.Molecule object, got %s '\n",
        "                        'instead' % type(mol))\n",
        "    if featurizer is None:\n",
        "        raise ValueError('featurizer must be set to make predistions for '\n",
        "                         'molecules')\n",
        "    if scale is None:\n",
        "        raise ValueError('scale must be set to make predistions')\n",
        "    prot_coords, prot_features = featurizer.get_features(mol)\n",
        "    centroid = prot_coords.mean(axis=0)\n",
        "    prot_coords -= centroid\n",
        "\n",
        "\n",
        "    resolution = 1. / scale\n",
        "    x = make_grid(prot_coords, prot_features,\n",
        "                              max_dist= max_dist,\n",
        "                              grid_resolution=resolution)\n",
        "    density = RAPID_Net_run3.predict(x)\n",
        "\n",
        "    origin = (centroid - max_dist)\n",
        "    step = np.array([1.0 / scale] * 3)\n",
        "\n",
        "    return density, origin, step\n",
        "\n",
        "def pocket_density_from_mol_RAPID_Net_run4(mol):\n",
        "    if not isinstance(mol, pybel.Molecule):\n",
        "        raise TypeError('mol should be a pybel.Molecule object, got %s '\n",
        "                        'instead' % type(mol))\n",
        "    if featurizer is None:\n",
        "        raise ValueError('featurizer must be set to make predistions for '\n",
        "                         'molecules')\n",
        "    if scale is None:\n",
        "        raise ValueError('scale must be set to make predistions')\n",
        "    prot_coords, prot_features = featurizer.get_features(mol)\n",
        "    centroid = prot_coords.mean(axis=0)\n",
        "    prot_coords -= centroid\n",
        "\n",
        "\n",
        "    resolution = 1. / scale\n",
        "    x = make_grid(prot_coords, prot_features,\n",
        "                              max_dist= max_dist,\n",
        "                              grid_resolution=resolution)\n",
        "    density = RAPID_Net_run4.predict(x)\n",
        "\n",
        "    origin = (centroid - max_dist)\n",
        "    step = np.array([1.0 / scale] * 3)\n",
        "\n",
        "    return density, origin, step\n",
        "\n",
        "def pocket_density_from_mol_RAPID_Net_run5(mol):\n",
        "    if not isinstance(mol, pybel.Molecule):\n",
        "        raise TypeError('mol should be a pybel.Molecule object, got %s '\n",
        "                        'instead' % type(mol))\n",
        "    if featurizer is None:\n",
        "        raise ValueError('featurizer must be set to make predistions for '\n",
        "                         'molecules')\n",
        "    if scale is None:\n",
        "        raise ValueError('scale must be set to make predistions')\n",
        "    prot_coords, prot_features = featurizer.get_features(mol)\n",
        "    centroid = prot_coords.mean(axis=0)\n",
        "    prot_coords -= centroid\n",
        "\n",
        "\n",
        "    resolution = 1. / scale\n",
        "    x = make_grid(prot_coords, prot_features,\n",
        "                              max_dist= max_dist,\n",
        "                              grid_resolution=resolution)\n",
        "    density = RAPID_Net_run5.predict(x)\n",
        "\n",
        "    origin = (centroid - max_dist)\n",
        "    step = np.array([1.0 / scale] * 3)\n",
        "\n",
        "    return density, origin, step\n",
        "\n",
        "from skimage.morphology import closing, label\n",
        "from skimage.segmentation import clear_border\n",
        "import numpy as np\n",
        "import scipy.ndimage as ndi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def minimal_pockets_segmentation(density1, density2, density3, density4, density5, threshold=threshold_input, min_size=10, scale=0.5, voters = min_vote_minimal):\n",
        "\n",
        "    voxel_size = (1 / scale) ** 3\n",
        "\n",
        "    bw1 = closing((density1[0] > threshold).any(axis=-1))\n",
        "    bw2 = closing((density2[0] > threshold).any(axis=-1))\n",
        "    bw3 = closing((density3[0] > threshold).any(axis=-1))\n",
        "    bw4 = closing((density4[0] > threshold).any(axis=-1))\n",
        "    bw5 = closing((density5[0] > threshold).any(axis=-1))\n",
        "\n",
        "\n",
        "    # Minimally-reported pockets, voted by at least one model\n",
        "    combined_bw = np.sum([bw1, bw2, bw3, bw4, bw5], axis=0) >= voters\n",
        "\n",
        "    # Apply morphological closing to reduce fragmentation\n",
        "    combined_bw = ndi.binary_closing(combined_bw, structure=np.ones((3, 3, 3)))\n",
        "\n",
        "    # Clear boundary-connected regions\n",
        "    cleared = clear_border(combined_bw)\n",
        "\n",
        "    # Label connected regions\n",
        "    label_image, num_labels = label(cleared, return_num=True)\n",
        "\n",
        "    for i in range(1, num_labels + 1):\n",
        "        pocket_idx = (label_image == i)\n",
        "        pocket_size = pocket_idx.sum() * voxel_size\n",
        "        if pocket_size < min_size:\n",
        "            label_image[np.where(pocket_idx)] = 0\n",
        "\n",
        "    return label_image\n",
        "\n",
        "def ensembled_pockets_segmentation(density1, density2, density3, density4, density5, threshold=threshold_input, min_size=50, scale=0.5, voters = min_vote_majority):\n",
        "\n",
        "    voxel_size = (1 / scale) ** 3\n",
        "\n",
        "    bw1 = closing((density1[0] > threshold).any(axis=-1))\n",
        "    bw2 = closing((density2[0] > threshold).any(axis=-1))\n",
        "    bw3 = closing((density3[0] > threshold).any(axis=-1))\n",
        "    bw4 = closing((density4[0] > threshold).any(axis=-1))\n",
        "    bw5 = closing((density5[0] > threshold).any(axis=-1))\n",
        "\n",
        "    # Majority-voted pockets, predicted by at least 3 models.\n",
        "    combined_bw = np.sum([bw1, bw2, bw3, bw4, bw5], axis=0) >= voters\n",
        "\n",
        "    # Apply morphological closing to reduce fragmentation\n",
        "    combined_bw = ndi.binary_closing(combined_bw, structure=np.ones((3, 3, 3)))\n",
        "\n",
        "    cleared = clear_border(combined_bw)\n",
        "\n",
        "    label_image, num_labels = label(cleared, return_num=True)\n",
        "\n",
        "    for i in range(1, num_labels + 1):\n",
        "        pocket_idx = (label_image == i)\n",
        "        pocket_size = pocket_idx.sum() * voxel_size\n",
        "        if pocket_size < min_size:\n",
        "            label_image[np.where(pocket_idx)] = 0\n",
        "\n",
        "    return label_image\n",
        "\n",
        "\n",
        "def save_pocket_mol2_RAPID_Net_Majority(mol, path, format, **pocket_kwargs):\n",
        "\n",
        "    density1, origin1, step1 = pocket_density_from_mol_RAPID_Net_run1(mol)\n",
        "    density1 = np.clip(density1, 0, 1)\n",
        "\n",
        "    density2, origin2, step2 = pocket_density_from_mol_RAPID_Net_run2(mol)\n",
        "    density2 = np.clip(density2, 0, 1)\n",
        "\n",
        "    density3, origin3, step3 = pocket_density_from_mol_RAPID_Net_run3(mol)\n",
        "    density3 = np.clip(density3, 0, 1)\n",
        "\n",
        "    density4, origin4, step4 = pocket_density_from_mol_RAPID_Net_run4(mol)\n",
        "    density4 = np.clip(density4, 0, 1)\n",
        "\n",
        "    density5, origin5, step5 = pocket_density_from_mol_RAPID_Net_run5(mol)\n",
        "    density5 = np.clip(density5, 0, 1)\n",
        "\n",
        "    pockets = ensembled_pockets_segmentation(density1, density2, density3, density4, density5, threshold=threshold_input, min_size=50, scale=0.5, voters = min_vote_majority)\n",
        "\n",
        "    i = 0\n",
        "    for pocket_label in range(1, pockets.max() + 1):\n",
        "        indices = np.argwhere(pockets == pocket_label).astype('float')\n",
        "        indices *= step1\n",
        "        indices += origin1\n",
        "        mol = openbabel.OBMol()\n",
        "        for idx in indices:\n",
        "            a = mol.NewAtom()\n",
        "            a.SetVector(float(idx[0]), float(idx[1]), float(idx[2]))\n",
        "        p_mol = pybel.Molecule(mol)\n",
        "\n",
        "\n",
        "        # Formatting threshold (e.g., 0.5 ‚Üí 05, 0.25 ‚Üí 025)\n",
        "        threshold_str = f\"{threshold_input:.2f}\".replace(\".\", \"\")\n",
        "        p_mol.write(format, f\"{path}/pocket_thr{threshold_str}_Majority{i}.{format}\")\n",
        "\n",
        "        i += 1\n",
        "\n",
        "def save_pocket_mol2_RAPID_Net_Minimal(mol, path, format, **pocket_kwargs):\n",
        "\n",
        "    density1, origin1, step1 = pocket_density_from_mol_RAPID_Net_run1(mol)\n",
        "    density1 = np.clip(density1, 0, 1)\n",
        "\n",
        "    density2, origin2, step2 = pocket_density_from_mol_RAPID_Net_run2(mol)\n",
        "    density2 = np.clip(density2, 0, 1)\n",
        "\n",
        "    density3, origin3, step3 = pocket_density_from_mol_RAPID_Net_run3(mol)\n",
        "    density3 = np.clip(density3, 0, 1)\n",
        "\n",
        "    density4, origin4, step4 = pocket_density_from_mol_RAPID_Net_run4(mol)\n",
        "    density4 = np.clip(density4, 0, 1)\n",
        "\n",
        "    density5, origin5, step5 = pocket_density_from_mol_RAPID_Net_run5(mol)\n",
        "    density5 = np.clip(density5, 0, 1)\n",
        "\n",
        "    pockets = minimal_pockets_segmentation(density1, density2, density3, density4, density5, threshold=threshold_input, min_size=10, scale=0.5, voters = min_vote_minimal)\n",
        "\n",
        "    i = 0\n",
        "    for pocket_label in range(1, pockets.max() + 1):\n",
        "        indices = np.argwhere(pockets == pocket_label).astype('float')\n",
        "        indices *= step1\n",
        "        indices += origin1\n",
        "        mol = openbabel.OBMol()\n",
        "        for idx in indices:\n",
        "            a = mol.NewAtom()\n",
        "            a.SetVector(float(idx[0]), float(idx[1]), float(idx[2]))\n",
        "        p_mol = pybel.Molecule(mol)\n",
        "\n",
        "        # Formatting threshold (e.g., 0.5 ‚Üí 05, 0.25 ‚Üí 025)\n",
        "        threshold_str = f\"{threshold_input:.2f}\".replace(\".\", \"\")\n",
        "        p_mol.write(format, f\"{path}/pocket_thr{threshold_str}_Minimal{i}.{format}\")\n",
        "\n",
        "        i += 1\n",
        "\n",
        "def save_pocket_mol2_RAPID_Net_Combined(mol, path, format, **pocket_kwargs):\n",
        "    # Combined function to predict both majority-voted and minority-reported pockets.\n",
        "    density1, origin1, step1 = pocket_density_from_mol_RAPID_Net_run1(mol)\n",
        "    density1 = np.clip(density1, 0, 1)\n",
        "\n",
        "    density2, origin2, step2 = pocket_density_from_mol_RAPID_Net_run2(mol)\n",
        "    density2 = np.clip(density2, 0, 1)\n",
        "\n",
        "    density3, origin3, step3 = pocket_density_from_mol_RAPID_Net_run3(mol)\n",
        "    density3 = np.clip(density3, 0, 1)\n",
        "\n",
        "    density4, origin4, step4 = pocket_density_from_mol_RAPID_Net_run4(mol)\n",
        "    density4 = np.clip(density4, 0, 1)\n",
        "\n",
        "    density5, origin5, step5 = pocket_density_from_mol_RAPID_Net_run5(mol)\n",
        "    density5 = np.clip(density5, 0, 1)\n",
        "\n",
        "    pockets_majority = ensembled_pockets_segmentation(density1, density2, density3, density4, density5, threshold=threshold_input, min_size=50, scale=0.5, voters = min_vote_majority)\n",
        "    pockets_minimal = minimal_pockets_segmentation(density1, density2, density3, density4, density5, threshold=threshold_input, min_size=10, scale=0.5, voters = min_vote_minimal)\n",
        "\n",
        "    def save_pockets(pockets, label):\n",
        "        i = 0\n",
        "        for pocket_label in range(1, pockets.max() + 1):\n",
        "            indices = np.argwhere(pockets == pocket_label).astype('float')\n",
        "            indices *= step1\n",
        "            indices += origin1\n",
        "            mol = openbabel.OBMol()\n",
        "            for idx in indices:\n",
        "                a = mol.NewAtom()\n",
        "                a.SetVector(float(idx[0]), float(idx[1]), float(idx[2]))\n",
        "            p_mol = pybel.Molecule(mol)\n",
        "\n",
        "            # Formatting threshold (e.g., 0.5 ‚Üí 05, 0.25 ‚Üí 025)\n",
        "            threshold_str = f\"{threshold_input:.2f}\".replace(\".\", \"\")\n",
        "            p_mol.write(format, f\"{path}/pocket_thr{threshold_str}_{label}{i}.{format}\")\n",
        "            i += 1\n",
        "\n",
        "    # Save both types of pockets\n",
        "    save_pockets(pockets_majority, \"Majority\")\n",
        "    save_pockets(pockets_minimal, \"Minimal\")\n",
        "\n",
        "def count_atoms_in_pdb(file_path):\n",
        "    \"\"\"Count atoms in PDB file to erase empty pockets\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return sum(1 for line in lines if line.startswith(\"ATOM\") or line.startswith(\"HETATM\"))\n",
        "\n",
        "def calculate_distance(atom1, atom2):\n",
        "    \"\"\"Calculates the distance between two atoms.\"\"\"\n",
        "    coord1 = np.array(atom1.get_coord())\n",
        "    coord2 = np.array(atom2.get_coord())\n",
        "    return np.linalg.norm(coord1 - coord2)\n",
        "\n",
        "def is_within_distance(protein_atoms, pocket_file, distance_threshold=20.0):\n",
        "    \"\"\"\n",
        "    Checks if every atom in the pocket file is within 'distance_threshold' √Ö\n",
        "    of at least one atom in the list of 'protein_atoms'.\n",
        "    If any pocket atom is farther than 'distance_threshold' √Ö from all protein atoms,\n",
        "    return False.\n",
        "    \"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    pocket_structure = parser.get_structure(\"Pocket\", pocket_file)\n",
        "\n",
        "    for pocket_model in pocket_structure:\n",
        "        for pocket_chain in pocket_model:\n",
        "            for pocket_residue in pocket_chain:\n",
        "                for pocket_atom in pocket_residue:\n",
        "                    # Check if this pocket_atom is within the threshold for ANY protein atom\n",
        "                    within_threshold = any(\n",
        "                        calculate_distance(pocket_atom, protein_atom) <= distance_threshold\n",
        "                        for protein_atom in protein_atoms\n",
        "                    )\n",
        "                    if not within_threshold:\n",
        "                        # Found a pocket atom that is NOT within threshold of any protein atom\n",
        "                        return False\n",
        "\n",
        "    # If we never returned False, it means all pocket atoms were within threshold\n",
        "    return True\n",
        "\n",
        "def remove_invalid_pockets(protein_file, folder_path, distance_threshold=8.0):\n",
        "    \"\"\"Removes empty pockets and those beyond the distance threshold from the protein.\"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    protein_structure = parser.get_structure(\"Protein\", protein_file)\n",
        "\n",
        "    # Extract all protein atoms\n",
        "    protein_atoms = [atom for model in protein_structure\n",
        "                     for chain in model\n",
        "                     for residue in chain\n",
        "                     for atom in residue]\n",
        "\n",
        "    remaining_pocket_files = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.startswith(\"pocket\") and file_name.endswith(\".pdb\"):\n",
        "            pocket_file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            # Check if the file is empty (zero atoms)\n",
        "            if count_atoms_in_pdb(pocket_file_path) == 0:\n",
        "                os.remove(pocket_file_path)\n",
        "                print(f\"Deleted empty pocket file: {file_name}\")\n",
        "                continue\n",
        "\n",
        "            # Check if the file is within the required distance\n",
        "            if not is_within_distance(protein_atoms, pocket_file_path, distance_threshold):\n",
        "                os.remove(pocket_file_path)\n",
        "                print(f\"Removed junk: {file_name}\")\n",
        "            else:\n",
        "                remaining_pocket_files.append(pocket_file_path)\n",
        "                print(f\"Retained pocket file: {file_name}\")\n",
        "\n",
        "    print(\"\\nFinal list of retained pocket files:\")\n",
        "    for file in remaining_pocket_files:\n",
        "        print(file)\n",
        "\n",
        "\n",
        "# Featurizer from tfbio package\n",
        "# https://gitlab.com/cheminfIBB/tfbio\n",
        "import os\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "import scipy.stats as stats\n",
        "import scipy.cluster.hierarchy\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from matplotlib import colors\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Draw, rdMolTransforms, rdDepictor, rdForceFieldHelpers\n",
        "from rdkit.Chem.Draw import rdMolDraw2D, IPythonConsole\n",
        "from IPython.display import SVG, Image\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from openbabel import pybel\n",
        "\n",
        "import pickle\n",
        "from math import ceil, sin, cos, sqrt, pi\n",
        "from itertools import combinations\n",
        "\n",
        "from statistics import mean, stdev\n",
        "\n",
        "class Featurizer():\n",
        "    \"\"\"Calcaulates atomic features for molecules. Features can encode atom type,\n",
        "    native pybel properties or any property defined with SMARTS patterns\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    FEATURE_NAMES: list of strings\n",
        "        Labels for features (in the same order as features)\n",
        "    NUM_ATOM_CLASSES: int\n",
        "        Number of atom codes\n",
        "    ATOM_CODES: dict\n",
        "        Dictionary mapping atomic numbers to codes\n",
        "    NAMED_PROPS: list of string\n",
        "        Names of atomic properties to retrieve from pybel.Atom object\n",
        "    CALLABLES: list of callables\n",
        "        Callables used to calculcate custom atomic properties\n",
        "    SMARTS: list of SMARTS strings\n",
        "        SMARTS patterns defining additional atomic properties\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, atom_codes=None, atom_labels=None,\n",
        "                 named_properties=None, save_molecule_codes=True,\n",
        "                 custom_properties=None, smarts_properties=None,\n",
        "                 smarts_labels=None):\n",
        "\n",
        "        \"\"\"Creates Featurizer with specified types of features. Elements of a\n",
        "        feature vector will be in a following order: atom type encoding\n",
        "        (defined by atom_codes), Pybel atomic properties (defined by\n",
        "        named_properties), molecule code (if present), custom atomic properties\n",
        "        (defined `custom_properties`), and additional properties defined with\n",
        "        SMARTS (defined with `smarts_properties`).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        atom_codes: dict, optional\n",
        "            Dictionary mapping atomic numbers to codes. It will be used for\n",
        "            one-hot encoging therefore if n different types are used, codes\n",
        "            shpuld be from 0 to n-1. Multiple atoms can have the same code,\n",
        "            e.g. you can use {6: 0, 7: 1, 8: 1} to encode carbons with [1, 0]\n",
        "            and nitrogens and oxygens with [0, 1] vectors. If not provided,\n",
        "            default encoding is used.\n",
        "        atom_labels: list of strings, optional\n",
        "            Labels for atoms codes. It should have the same length as the\n",
        "            number of used codes, e.g. for `atom_codes={6: 0, 7: 1, 8: 1}` you\n",
        "            should provide something like ['C', 'O or N']. If not specified\n",
        "            labels 'atom0', 'atom1' etc are used. If `atom_codes` is not\n",
        "            specified this argument is ignored.\n",
        "        named_properties: list of strings, optional\n",
        "            Names of atomic properties to retrieve from pybel.Atom object. If\n",
        "            not specified ['hyb', 'heavyvalence', 'heterovalence',\n",
        "            'partialcharge'] is used.\n",
        "        save_molecule_codes: bool, optional (default True)\n",
        "            If set to True, there will be an additional feature to save\n",
        "            molecule code. It is usefeul when saving molecular complex in a\n",
        "            single array.\n",
        "        custom_properties: list of callables, optional\n",
        "            Custom functions to calculate atomic properties. Each element of\n",
        "            this list should be a callable that takes pybel.Atom object and\n",
        "            returns a float. If callable has `__name__` property it is used as\n",
        "            feature label. Otherwise labels 'func<i>' etc are used, where i is\n",
        "            the index in `custom_properties` list.\n",
        "        smarts_properties: list of strings, optional\n",
        "            Additional atomic properties defined with SMARTS patterns. These\n",
        "            patterns should match a single atom. If not specified, deafult\n",
        "            patterns are used.\n",
        "        smarts_labels: list of strings, optional\n",
        "            Labels for properties defined with SMARTS. Should have the same\n",
        "            length as `smarts_properties`. If not specified labels 'smarts0',\n",
        "            'smarts1' etc are used. If `smarts_properties` is not specified\n",
        "            this argument is ignored.\n",
        "        \"\"\"\n",
        "\n",
        "        # Remember namse of all features in the correct order\n",
        "        self.FEATURE_NAMES = []\n",
        "\n",
        "        if atom_codes is not None:\n",
        "            if not isinstance(atom_codes, dict):\n",
        "                raise TypeError('Atom codes should be dict, got %s instead'\n",
        "                                % type(atom_codes))\n",
        "            codes = set(atom_codes.values())\n",
        "            for i in range(len(codes)):\n",
        "                if i not in codes:\n",
        "                    raise ValueError('Incorrect atom code %s' % i)\n",
        "\n",
        "            self.NUM_ATOM_CLASSES = len(codes)\n",
        "            self.ATOM_CODES = atom_codes\n",
        "            if atom_labels is not None:\n",
        "                if len(atom_labels) != self.NUM_ATOM_CLASSES:\n",
        "                    raise ValueError('Incorrect number of atom labels: '\n",
        "                                     '%s instead of %s'\n",
        "                                     % (len(atom_labels), self.NUM_ATOM_CLASSES))\n",
        "            else:\n",
        "                atom_labels = ['atom%s' % i for i in range(self.NUM_ATOM_CLASSES)]\n",
        "            self.FEATURE_NAMES += atom_labels\n",
        "        else:\n",
        "            self.ATOM_CODES = {}\n",
        "\n",
        "            metals = ([3, 4, 11, 12, 13] + list(range(19, 32))\n",
        "                      + list(range(37, 51)) + list(range(55, 84))\n",
        "                      + list(range(87, 104)))\n",
        "\n",
        "            # List of tuples (atomic_num, class_name) with atom types to encode.\n",
        "            atom_classes = [\n",
        "                (5, 'B'),\n",
        "                (6, 'C'),\n",
        "                (7, 'N'),\n",
        "                (8, 'O'),\n",
        "                (15, 'P'),\n",
        "                (16, 'S'),\n",
        "                (34, 'Se'),\n",
        "                ([9, 17, 35, 53], 'halogen'),\n",
        "                (metals, 'metal')\n",
        "            ]\n",
        "\n",
        "            for code, (atom, name) in enumerate(atom_classes):\n",
        "                if type(atom) is list:\n",
        "                    for a in atom:\n",
        "                        self.ATOM_CODES[a] = code\n",
        "                else:\n",
        "                    self.ATOM_CODES[atom] = code\n",
        "                self.FEATURE_NAMES.append(name)\n",
        "\n",
        "            self.NUM_ATOM_CLASSES = len(atom_classes)\n",
        "\n",
        "        if named_properties is not None:\n",
        "            if not isinstance(named_properties, (list, tuple, np.ndarray)):\n",
        "                raise TypeError('named_properties must be a list')\n",
        "            allowed_props = [prop for prop in dir(pybel.Atom)\n",
        "                             if not prop.startswith('__')]\n",
        "            for prop_id, prop in enumerate(named_properties):\n",
        "                if prop not in allowed_props:\n",
        "                    raise ValueError(\n",
        "                        'named_properties must be in pybel.Atom attributes,'\n",
        "                        ' %s was given at position %s' % (prop_id, prop)\n",
        "                    )\n",
        "            self.NAMED_PROPS = named_properties\n",
        "        else:\n",
        "            # pybel.Atom properties to save\n",
        "            self.NAMED_PROPS = ['hyb', 'heavydegree', 'heterodegree',\n",
        "                                'partialcharge']\n",
        "        self.FEATURE_NAMES += self.NAMED_PROPS\n",
        "\n",
        "        if not isinstance(save_molecule_codes, bool):\n",
        "            raise TypeError('save_molecule_codes should be bool, got %s '\n",
        "                            'instead' % type(save_molecule_codes))\n",
        "        self.save_molecule_codes = save_molecule_codes\n",
        "        if save_molecule_codes:\n",
        "            # Remember if an atom belongs to the ligand or to the protein\n",
        "            self.FEATURE_NAMES.append('molcode')\n",
        "\n",
        "        self.CALLABLES = []\n",
        "        if custom_properties is not None:\n",
        "            for i, func in enumerate(custom_properties):\n",
        "                if not callable(func):\n",
        "                    raise TypeError('custom_properties should be list of'\n",
        "                                    ' callables, got %s instead' % type(func))\n",
        "                name = getattr(func, '__name__', '')\n",
        "                if name == '':\n",
        "                    name = 'func%s' % i\n",
        "                self.CALLABLES.append(func)\n",
        "                self.FEATURE_NAMES.append(name)\n",
        "\n",
        "        if smarts_properties is None:\n",
        "            # SMARTS definition for other properties\n",
        "            self.SMARTS = [\n",
        "                '[#6+0!$(*~[#7,#8,F]),SH0+0v2,s+0,S^3,Cl+0,Br+0,I+0]',\n",
        "                '[a]',\n",
        "                '[!$([#1,#6,F,Cl,Br,I,o,s,nX3,#7v5,#15v5,#16v4,#16v6,*+1,*+2,*+3])]',\n",
        "                '[!$([#6,H0,-,-2,-3]),$([!H0;#7,#8,#9])]',\n",
        "                '[r]'\n",
        "            ]\n",
        "            smarts_labels = ['hydrophobic', 'aromatic', 'acceptor', 'donor',\n",
        "                             'ring']\n",
        "        elif not isinstance(smarts_properties, (list, tuple, np.ndarray)):\n",
        "            raise TypeError('smarts_properties must be a list')\n",
        "        else:\n",
        "            self.SMARTS = smarts_properties\n",
        "\n",
        "        if smarts_labels is not None:\n",
        "            if len(smarts_labels) != len(self.SMARTS):\n",
        "                raise ValueError('Incorrect number of SMARTS labels: %s'\n",
        "                                 ' instead of %s'\n",
        "                                 % (len(smarts_labels), len(self.SMARTS)))\n",
        "        else:\n",
        "            smarts_labels = ['smarts%s' % i for i in range(len(self.SMARTS))]\n",
        "\n",
        "        # Compile patterns\n",
        "        self.compile_smarts()\n",
        "        self.FEATURE_NAMES += smarts_labels\n",
        "\n",
        "    def compile_smarts(self):\n",
        "        self.__PATTERNS = []\n",
        "        for smarts in self.SMARTS:\n",
        "            self.__PATTERNS.append(pybel.Smarts(smarts))\n",
        "\n",
        "    def encode_num(self, atomic_num):\n",
        "        \"\"\"Encode atom type with a binary vector. If atom type is not included in\n",
        "        the `atom_classes`, its encoding is an all-zeros vector.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        atomic_num: int\n",
        "            Atomic number\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        encoding: np.ndarray\n",
        "            Binary vector encoding atom type (one-hot or null).\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(atomic_num, int):\n",
        "            raise TypeError('Atomic number must be int, %s was given'\n",
        "                            % type(atomic_num))\n",
        "\n",
        "        encoding = np.zeros(self.NUM_ATOM_CLASSES)\n",
        "        try:\n",
        "            encoding[self.ATOM_CODES[atomic_num]] = 1.0\n",
        "        except:\n",
        "            pass\n",
        "        return encoding\n",
        "\n",
        "    def find_smarts(self, molecule):\n",
        "        \"\"\"Find atoms that match SMARTS patterns.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        molecule: pybel.Molecule\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        features: np.ndarray\n",
        "            NxM binary array, where N is the number of atoms in the `molecule`\n",
        "            and M is the number of patterns. `features[i, j]` == 1.0 if i'th\n",
        "            atom has j'th property\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(molecule, pybel.Molecule):\n",
        "            raise TypeError('molecule must be pybel.Molecule object, %s was given'\n",
        "                            % type(molecule))\n",
        "\n",
        "        features = np.zeros((len(molecule.atoms), len(self.__PATTERNS)))\n",
        "\n",
        "        for (pattern_id, pattern) in enumerate(self.__PATTERNS):\n",
        "            atoms_with_prop = np.array(list(*zip(*pattern.findall(molecule))),\n",
        "                                       dtype=int) - 1\n",
        "            features[atoms_with_prop, pattern_id] = 1.0\n",
        "        return features\n",
        "\n",
        "    def get_features(self, molecule, molcode=None):\n",
        "        \"\"\"Get coordinates and features for all heavy atoms in the molecule.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        molecule: pybel.Molecule\n",
        "        molcode: float, optional\n",
        "            Molecule type. You can use it to encode whether an atom belongs to\n",
        "            the ligand (1.0) or to the protein (-1.0) etc.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        coords: np.ndarray, shape = (N, 3)\n",
        "            Coordinates of all heavy atoms in the `molecule`.\n",
        "        features: np.ndarray, shape = (N, F)\n",
        "            Features of all heavy atoms in the `molecule`: atom type\n",
        "            (one-hot encoding), pybel.Atom attributes, type of a molecule\n",
        "            (e.g protein/ligand distinction), and other properties defined with\n",
        "            SMARTS patterns\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(molecule, pybel.Molecule):\n",
        "            raise TypeError('molecule must be pybel.Molecule object,'\n",
        "                            ' %s was given' % type(molecule))\n",
        "        if molcode is None:\n",
        "            if self.save_molecule_codes is True:\n",
        "                raise ValueError('save_molecule_codes is set to True,'\n",
        "                                 ' you must specify code for the molecule')\n",
        "        elif not isinstance(molcode, (float, int)):\n",
        "            raise TypeError('motlype must be float, %s was given'\n",
        "                            % type(molcode))\n",
        "\n",
        "        coords = []\n",
        "        features = []\n",
        "        heavy_atoms = []\n",
        "\n",
        "        for i, atom in enumerate(molecule):\n",
        "            # ignore hydrogens and dummy atoms (they have atomicnum set to 0)\n",
        "            if atom.atomicnum > 1:\n",
        "                heavy_atoms.append(i)\n",
        "                coords.append(atom.coords)\n",
        "\n",
        "                features.append(np.concatenate((\n",
        "                    self.encode_num(atom.atomicnum),\n",
        "                    [atom.__getattribute__(prop) for prop in self.NAMED_PROPS],\n",
        "                    [func(atom) for func in self.CALLABLES],\n",
        "                )))\n",
        "\n",
        "        coords = np.array(coords, dtype=np.float32)\n",
        "        features = np.array(features, dtype=np.float32)\n",
        "        if self.save_molecule_codes:\n",
        "            features = np.hstack((features,\n",
        "                                  molcode * np.ones((len(features), 1))))\n",
        "        features = np.hstack([features,\n",
        "                              self.find_smarts(molecule)[heavy_atoms]])\n",
        "\n",
        "        if np.isnan(features).any():\n",
        "            raise RuntimeError('Got NaN when calculating features')\n",
        "\n",
        "        return coords, features\n",
        "\n",
        "    def to_pickle(self, fname='featurizer.pkl'):\n",
        "        \"\"\"Save featurizer in a given file. Featurizer can be restored with\n",
        "        `from_pickle` method.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fname: str, optional\n",
        "           Path to file in which featurizer will be saved\n",
        "        \"\"\"\n",
        "\n",
        "        # patterns can't be pickled, we need to temporarily remove them\n",
        "        patterns = self.__PATTERNS[:]\n",
        "        del self.__PATTERNS\n",
        "        try:\n",
        "            with open(fname, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "        finally:\n",
        "            self.__PATTERNS = patterns[:]\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pickle(fname):\n",
        "        \"\"\"Load pickled featurizer from a given file\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fname: str, optional\n",
        "           Path to file with saved featurizer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        featurizer: Featurizer object\n",
        "           Loaded featurizer\n",
        "        \"\"\"\n",
        "        with open(fname, 'rb') as f:\n",
        "            featurizer = pickle.load(f)\n",
        "        featurizer.compile_smarts()\n",
        "        return featurizer\n",
        "\n",
        "\n",
        "def rotation_matrix(axis, theta):\n",
        "    \"\"\"Counterclockwise rotation about a given axis by theta radians\"\"\"\n",
        "\n",
        "    if not isinstance(axis, (np.ndarray, list, tuple)):\n",
        "        raise TypeError('axis must be an array of floats of shape (3,)')\n",
        "    try:\n",
        "        axis = np.asarray(axis, dtype=np.float64)\n",
        "    except ValueError:\n",
        "        raise ValueError('axis must be an array of floats of shape (3,)')\n",
        "\n",
        "    if axis.shape != (3,):\n",
        "        raise ValueError('axis must be an array of floats of shape (3,)')\n",
        "\n",
        "    if not isinstance(theta, (float, int)):\n",
        "        raise TypeError('theta must be a float')\n",
        "\n",
        "    axis = axis / sqrt(np.dot(axis, axis))\n",
        "    a = cos(theta / 2.0)\n",
        "    b, c, d = -axis * sin(theta / 2.0)\n",
        "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
        "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
        "    return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
        "                     [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
        "                     [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
        "\n",
        "\n",
        "# Create matrices for all possible 90* rotations of a box\n",
        "ROTATIONS = [rotation_matrix([1, 1, 1], 0)]\n",
        "\n",
        "# about X, Y and Z - 9 rotations\n",
        "for a1 in range(3):\n",
        "    for t in range(1, 4):\n",
        "        axis = np.zeros(3)\n",
        "        axis[a1] = 1\n",
        "        theta = t * pi / 2.0\n",
        "        ROTATIONS.append(rotation_matrix(axis, theta))\n",
        "\n",
        "# about each face diagonal - 6 rotations\n",
        "for (a1, a2) in combinations(range(3), 2):\n",
        "    axis = np.zeros(3)\n",
        "    axis[[a1, a2]] = 1.0\n",
        "    theta = pi\n",
        "    ROTATIONS.append(rotation_matrix(axis, theta))\n",
        "    axis[a2] = -1.0\n",
        "    ROTATIONS.append(rotation_matrix(axis, theta))\n",
        "\n",
        "# about each space diagonal - 8 rotations\n",
        "for t in [1, 2]:\n",
        "    theta = t * 2 * pi / 3\n",
        "    axis = np.ones(3)\n",
        "    ROTATIONS.append(rotation_matrix(axis, theta))\n",
        "    for a1 in range(3):\n",
        "        axis = np.ones(3)\n",
        "        axis[a1] = -1\n",
        "        ROTATIONS.append(rotation_matrix(axis, theta))\n",
        "\n",
        "\n",
        "def rotate(coords, rotation):\n",
        "    \"\"\"Rotate coordinates by a given rotation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    coords: array-like, shape (N, 3)\n",
        "        Arrays with coordinates and features for each atoms.\n",
        "    rotation: int or array-like, shape (3, 3)\n",
        "        Rotation to perform. You can either select predefined rotation by\n",
        "        giving its index or specify rotation matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    coords: np.ndarray, shape = (N, 3)\n",
        "        Rotated coordinates.\n",
        "    \"\"\"\n",
        "\n",
        "    global ROTATIONS\n",
        "\n",
        "    if not isinstance(coords, (np.ndarray, list, tuple)):\n",
        "        raise TypeError('coords must be an array of floats of shape (N, 3)')\n",
        "    try:\n",
        "        coords = np.asarray(coords, dtype=np.float64)\n",
        "    except ValueError:\n",
        "        raise ValueError('coords must be an array of floats of shape (N, 3)')\n",
        "    shape = coords.shape\n",
        "    if len(shape) != 2 or shape[1] != 3:\n",
        "        raise ValueError('coords must be an array of floats of shape (N, 3)')\n",
        "\n",
        "    if isinstance(rotation, int):\n",
        "        if rotation >= 0 and rotation < len(ROTATIONS):\n",
        "            return np.dot(coords, ROTATIONS[rotation])\n",
        "        else:\n",
        "            raise ValueError('Invalid rotation number %s!' % rotation)\n",
        "    elif isinstance(rotation, np.ndarray) and rotation.shape == (3, 3):\n",
        "        return np.dot(coords, rotation)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Invalid rotation %s!' % rotation)\n",
        "\n",
        "def make_grid(coords, features, grid_resolution=1.0, max_dist=10.0):\n",
        "    \"\"\"Convert atom coordinates and features represented as 2D arrays into a\n",
        "    fixed-sized 3D box.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    coords, features: array-likes, shape (N, 3) and (N, F)\n",
        "        Arrays with coordinates and features for each atoms.\n",
        "    grid_resolution: float, optional\n",
        "        Resolution of a grid (in Angstroms).\n",
        "    max_dist: float, optional\n",
        "        Maximum distance between atom and box center. Resulting box has size of\n",
        "        2*`max_dist`+1 Angstroms and atoms that are too far away are not\n",
        "        included.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    coords: np.ndarray, shape = (M, M, M, F)\n",
        "        4D array with atom properties distributed in 3D space. M is equal to\n",
        "        2 * `max_dist` / `grid_resolution` + 1\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        coords = np.asarray(coords, dtype=np.float64)\n",
        "    except ValueError:\n",
        "        raise ValueError('coords must be an array of floats of shape (N, 3)')\n",
        "    c_shape = coords.shape\n",
        "    if len(c_shape) != 2 or c_shape[1] != 3:\n",
        "        raise ValueError('coords must be an array of floats of shape (N, 3)')\n",
        "\n",
        "    N = len(coords)\n",
        "    try:\n",
        "        features = np.asarray(features, dtype=np.float64)\n",
        "    except ValueError:\n",
        "        raise ValueError('features must be an array of floats of shape (N, F)')\n",
        "    f_shape = features.shape\n",
        "    if len(f_shape) != 2 or f_shape[0] != N:\n",
        "        raise ValueError('features must be an array of floats of shape (N, F)')\n",
        "\n",
        "    if not isinstance(grid_resolution, (float, int)):\n",
        "        raise TypeError('grid_resolution must be float')\n",
        "    if grid_resolution <= 0:\n",
        "        raise ValueError('grid_resolution must be positive')\n",
        "\n",
        "    if not isinstance(max_dist, (float, int)):\n",
        "        raise TypeError('max_dist must be float')\n",
        "    if max_dist <= 0:\n",
        "        raise ValueError('max_dist must be positive')\n",
        "\n",
        "    num_features = f_shape[1]\n",
        "    max_dist = float(max_dist)\n",
        "    grid_resolution = float(grid_resolution)\n",
        "\n",
        "    box_size = ceil(2 * max_dist / grid_resolution + 1)\n",
        "\n",
        "    # move all atoms to the neares grid point\n",
        "    grid_coords = (coords + max_dist) / grid_resolution\n",
        "    grid_coords = grid_coords.round().astype(int)\n",
        "\n",
        "    # remove atoms outside the box\n",
        "    in_box = ((grid_coords >= 0) & (grid_coords < box_size)).all(axis=1)\n",
        "    grid = np.zeros((1, box_size, box_size, box_size, num_features),\n",
        "                    dtype=np.float32)\n",
        "    for (x, y, z), f in zip(grid_coords[in_box], features[in_box]):\n",
        "        grid[0, x, y, z] += f\n",
        "\n",
        "    return grid\n",
        "\n",
        "featurizer = Featurizer(save_molecule_codes = False)\n",
        "\n",
        "scale=0.5\n",
        "max_dist=35\n",
        "file_format = 'pdb'\n",
        "\n",
        "grid_resolution=1.0\n",
        "scale=0.5\n",
        "grid_size=36\n",
        "\n",
        "resolution = 1. / scale\n",
        "print(\"\\nü¶Å Preparing RAPID-Net model for launch...\")\n",
        "\n",
        "# Download model weights\n",
        "model_files = [\n",
        "    \"Soft_Dice_Relu_5020_Run_1.keras\",\n",
        "    \"Soft_Dice_Relu_5020_Run_2.keras\",\n",
        "    \"Soft_Dice_Relu_5020_Run_3.keras\",\n",
        "    \"Soft_Dice_Relu_5020_Run_4.keras\",\n",
        "    \"Soft_Dice_Relu_5020_Run_5.keras\"\n",
        "]\n",
        "\n",
        "print(\"üîÑ Downloading model weights from Zenodo...\")\n",
        "for model_file in model_files:\n",
        "    !wget -q https://zenodo.org/record/14796981/files/{model_file} -O {model_file}\n",
        "    if os.path.exists(f\"/content/{model_file}\"):\n",
        "        print(f\"‚úÖ {model_file} downloaded successfully!\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {model_file} not found!\")\n",
        "\n",
        "# Initialize RAPID-Net models\n",
        "print(\"üîÑ Initializing RAPID-Net model...\")\n",
        "try:\n",
        "    RAPID_Net_run1 = RAPID_Net()\n",
        "    RAPID_Net_run2 = RAPID_Net()\n",
        "    RAPID_Net_run3 = RAPID_Net()\n",
        "    RAPID_Net_run4 = RAPID_Net()\n",
        "    RAPID_Net_run5 = RAPID_Net()\n",
        "    print(\"‚úÖ RAPID-Net model initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing RAPID-Net models: {e}\")\n",
        "\n",
        "# Load weights into the models\n",
        "print(\"üîÑ Loading model weights...\")\n",
        "try:\n",
        "    RAPID_Net_run1.load_weights('/content/Soft_Dice_Relu_5020_Run_1.keras')\n",
        "    print(\"‚úÖ Weights loaded for RAPID_Net_run1!\")\n",
        "\n",
        "    RAPID_Net_run2.load_weights('/content/Soft_Dice_Relu_5020_Run_2.keras')\n",
        "    print(\"‚úÖ Weights loaded for RAPID_Net_run2!\")\n",
        "\n",
        "    RAPID_Net_run3.load_weights('/content/Soft_Dice_Relu_5020_Run_3.keras')\n",
        "    print(\"‚úÖ Weights loaded for RAPID_Net_run3!\")\n",
        "\n",
        "    RAPID_Net_run4.load_weights('/content/Soft_Dice_Relu_5020_Run_4.keras')\n",
        "    print(\"‚úÖ Weights loaded for RAPID_Net_run4!\")\n",
        "\n",
        "    RAPID_Net_run5.load_weights('/content/Soft_Dice_Relu_5020_Run_5.keras')\n",
        "    print(\"‚úÖ Weights loaded for RAPID_Net_run5!\")\n",
        "\n",
        "    print(\"\\nüéâ RAPID-Net model ü¶Å unleashed and ready for pocket prediction! üöÄ\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model weights: {e}\")"
      ],
      "metadata": {
        "id": "IAEKGNoLDzCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üõ† Create a Repository for Your Protein, Predict Pockets, and Prepare Protein for Docking** { display-mode: \"form\" }\n",
        "# @markdown Enter the **protein name** and upload the file.\n",
        "\n",
        "# @markdown In this case, it is **ab5_ckfix.pdb**\n",
        "\n",
        "from pymol import cmd\n",
        "\n",
        "Job_name = \"ABHD5\"  # @param {type: \"string\"}\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "from openbabel import pybel\n",
        "\n",
        "Prediction_Type = \"Both Majority and Minimal\"  # @param [\"Majority\", \"Minimal\", \"Both Majority and Minimal\"]\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "from openbabel import pybel\n",
        "\n",
        "# Ensure a valid name is provided\n",
        "invalid_chars = '^<>/\\\\{}[]~`$ '\n",
        "assert Job_name, '‚ö†Ô∏è Please provide a name for the protein!'\n",
        "assert not set(invalid_chars).intersection(Job_name), '‚ö†Ô∏è Name contains disallowed characters!'\n",
        "\n",
        "# Create the protein repository (directory)\n",
        "WRK_DIR = os.path.join(os.getcwd(), Job_name)\n",
        "os.makedirs(WRK_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"‚úÖ Protein Repository Created: `{Job_name}`\")\n",
        "print(f\"üìÇ Directory: `{WRK_DIR}`\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Upload protein file\n",
        "print(\"\\nüì§ Please upload your protein file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded file to the created directory\n",
        "uploaded_filename = list(uploaded.keys())[0]  # Ensure only one file is processed\n",
        "file_path = os.path.join(WRK_DIR, uploaded_filename)\n",
        "os.rename(uploaded_filename, file_path)\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(f\"‚úÖ File Uploaded Successfully!\")\n",
        "print(f\"üìÑ File Name: `{uploaded_filename}`\")\n",
        "print(f\"üìÅ Stored in: `{WRK_DIR}`\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Define paths and formats\n",
        "protein_file = os.path.join(WRK_DIR, uploaded_filename)\n",
        "file_format = 'pdb'\n",
        "output_format = 'pdb'\n",
        "\n",
        "print(\"\\n‚è≥ Processing Protein File... Please wait.\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Read the molecular structure\n",
        "try:\n",
        "    protein_mol = next(pybel.readfile(file_format, protein_file))\n",
        "    print(\"‚úÖ Molecule successfully loaded from the PDB file!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading molecule: {e}\")\n",
        "    protein_mol = None  # Prevent further processing if failed\n",
        "\n",
        "# Apply the pocket prediction functions based on user choice\n",
        "if protein_mol:\n",
        "    print(\"\\nüîÑ Predicting Pockets...\")\n",
        "    try:\n",
        "        if Prediction_Type == \"Majority\":\n",
        "            save_pocket_mol2_RAPID_Net_Majority(protein_mol, WRK_DIR, output_format)\n",
        "            print(\"‚úÖ Majority-voted pockets generated successfully!\")\n",
        "        elif Prediction_Type == \"Minimal\":\n",
        "            save_pocket_mol2_RAPID_Net_Minimal(protein_mol, WRK_DIR, output_format)\n",
        "            print(\"‚úÖ Minimally-reporting pockets generated successfully!\")\n",
        "        else:\n",
        "            save_pocket_mol2_RAPID_Net_Combined(protein_mol, WRK_DIR, output_format)\n",
        "            print(\"‚úÖ Both Majority and Minimal pockets generated successfully!\")\n",
        "\n",
        "        remove_invalid_pockets(protein_file, WRK_DIR, distance_threshold=20.0)\n",
        "        print(\"‚úÖ Invalid pockets filtered out\")\n",
        "\n",
        "        print(\"\\nüéâ Processing Complete! All outputs saved in:\")\n",
        "        print(f\"üìÅ `{WRK_DIR}`\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during pocket prediction: {e}\")\n",
        "\n",
        "\n",
        "# Function to find the bounding box and generate Vina config file\n",
        "def process_pocket(pocket_path, pocket_name, thresholds, subfolder_path):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(pocket_name, pocket_path)\n",
        "\n",
        "    # Get all atom coordinates\n",
        "    atoms = [atom for atom in structure.get_atoms()]\n",
        "    coords = [atom.coord for atom in atoms]\n",
        "    coords = np.array(coords)\n",
        "\n",
        "    max_coords = np.max(coords, axis=0)\n",
        "    min_coords = np.min(coords, axis=0)\n",
        "\n",
        "    center = (max_coords + min_coords) / 2\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        DELTAx = (max_coords[0] - min_coords[0]) + 2 * threshold\n",
        "        DELTAy = (max_coords[1] - min_coords[1]) + 2 * threshold\n",
        "        DELTAz = (max_coords[2] - min_coords[2]) + 2 * threshold\n",
        "\n",
        "        # Create config content\n",
        "        config_content = f\"\"\"\\\n",
        "center_x = {center[0]}\n",
        "center_y = {center[1]}\n",
        "center_z = {center[2]}\n",
        "\n",
        "size_x = {DELTAx}\n",
        "size_y = {DELTAy}\n",
        "size_z = {DELTAz}\n",
        "\"\"\"\n",
        "        config_filename = f\"{pocket_name}_config_{threshold}.txt\"\n",
        "        config_file_path = os.path.join(subfolder_path, config_filename)\n",
        "\n",
        "        # Write config file\n",
        "        with open(config_file_path, 'w') as config_file:\n",
        "            config_file.write(config_content)\n",
        "\n",
        "        # Create directory for docking results\n",
        "        result_dir_name = f\"{pocket_name}_results_{threshold}\"\n",
        "        result_dir_path = os.path.join(subfolder_path, result_dir_name)\n",
        "        os.makedirs(result_dir_path, exist_ok=True)\n",
        "\n",
        "        print(f\"Generated config file: {config_file_path}\")\n",
        "        print(f\"Created results directory: {result_dir_path}\")\n",
        "\n",
        "# @markdown Specify the search grids.\n",
        "# @markdown - Provide a **majority_thresholds_input** to have a list of search grids for majority-voted pockets.\n",
        "# @markdown - Provide a **minority_thresholds_input** to have a list of search grids for majority-voted pockets.\n",
        "\n",
        "majority_thresholds_input = \"2, 5\"  # @param {type:\"string\"}\n",
        "\n",
        "# Convert input to a list of integers, ignoring invalid entries\n",
        "majority_thresholds = [int(thr_maj.strip()) for thr_maj in majority_thresholds_input.split(\",\") if thr_maj.strip().isdigit()]\n",
        "\n",
        "minority_thresholds_input = \"2\"  # @param {type:\"string\"}\n",
        "\n",
        "# Convert input to a list of integers, ignoring invalid entries\n",
        "minimal_thresholds = [int(thr_min.strip()) for thr_min in minority_thresholds_input.split(\",\") if thr_min.strip().isdigit()]\n",
        "\n",
        "minimal_pocket_files = [\n",
        "    os.path.join(WRK_DIR, file) for file in os.listdir(WRK_DIR)\n",
        "    if file.startswith(\"pocket\") and \"Minimal\" in file and file.endswith(\".pdb\")\n",
        "]\n",
        "\n",
        "for pocket_file in minimal_pocket_files:\n",
        "    pocket_path = os.path.join(WRK_DIR, pocket_file)\n",
        "    pocket_name = os.path.splitext(pocket_file)[0]\n",
        "    process_pocket(pocket_path, pocket_name, minimal_thresholds, WRK_DIR)\n",
        "\n",
        "majority_pocket_files = [\n",
        "    os.path.join(WRK_DIR, file) for file in os.listdir(WRK_DIR)\n",
        "    if file.startswith(\"pocket\") and \"Majority\" in file and file.endswith(\".pdb\")\n",
        "]\n",
        "\n",
        "for pocket_file in majority_pocket_files:\n",
        "    pocket_path = os.path.join(WRK_DIR, pocket_file)\n",
        "    pocket_name = os.path.splitext(pocket_file)[0]\n",
        "    process_pocket(pocket_path, pocket_name, majority_thresholds, WRK_DIR)\n",
        "\n",
        "def add_hydrogens(input_file, output_file):\n",
        "    reduce_executable = '/usr/local/bin/reduce'\n",
        "    het_dict_path = '/usr/local/reduce_wwPDB_het_dict.txt'\n",
        "\n",
        "    if not os.path.isfile(reduce_executable):\n",
        "        raise FileNotFoundError(f\"Reduce executable not found at {reduce_executable}\")\n",
        "\n",
        "    if not os.path.isfile(het_dict_path):\n",
        "        raise FileNotFoundError(f\"HET dictionary not found at {het_dict_path}\")\n",
        "\n",
        "    reduce_cmd = f\"{reduce_executable} -BUILD -DB {het_dict_path} {input_file} > {output_file}\"\n",
        "    try:\n",
        "        result = subprocess.run(reduce_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        print(f\"Hydrogens added to {input_file} successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error processing {input_file}: {e}\")\n",
        "        print(f\"Standard Output: {e.stdout.decode()}\")\n",
        "        print(f\"Standard Error: {e.stderr.decode()}\")\n",
        "\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from openbabel import pybel\n",
        "from rdkit.Chem import AddHs, MolFromMolFile\n",
        "\n",
        "# Check if Reduce repository is already cloned\n",
        "if not os.path.exists(\"/content/reduce\"):\n",
        "    print(\"\\nüîÑ Installing Reduce and dependencies to add exlicit hydrogens to the protein...\")\n",
        "    subprocess.run(\"apt-get update && apt-get install -y build-essential cmake\", shell=True, check=True)\n",
        "    subprocess.run(\"git clone https://github.com/rlabduke/reduce.git\", shell=True, check=True)\n",
        "    subprocess.run(\"mkdir -p /root/build/reduce && cmake -S /content/reduce -B /root/build/reduce\", shell=True, check=True)\n",
        "    subprocess.run(\"cmake --build /root/build/reduce --target install\", shell=True, check=True)\n",
        "    subprocess.run(\"sudo cp /content/reduce/reduce_wwPDB_het_dict.txt /usr/local/reduce_wwPDB_het_dict.txt\", shell=True, check=True)\n",
        "    print(\"‚úÖ Reduce installed successfully!\")\n",
        "else:\n",
        "    print(\"‚úÖ Reduce repository already exists, skipping installation!\")\n",
        "\n",
        "\n",
        "protein_file_reduce = os.path.join(os.path.dirname(protein_file), os.path.basename(protein_file).replace('.pdb', '_reduce.pdb'))\n",
        "add_hydrogens(protein_file, protein_file_reduce)\n",
        "\n",
        "print(\"‚úÖ Protonation completed with Reduce!\")\n",
        "\n",
        "# Define output path for cleaned file\n",
        "prot_pdb_rm_bad_path = protein_file_reduce.replace(\"_reduce.pdb\", \"_reduce_rm_bad.pdb\")\n",
        "\n",
        "# Function to remove unwanted atoms using PyMOL\n",
        "def _remove_bad_atoms(file_in: str, file_out: str):\n",
        "    cmd.reinitialize()\n",
        "\n",
        "    if not os.path.exists(file_in):\n",
        "        raise FileNotFoundError(f\"‚ùå Input file not found: {file_in}\")\n",
        "\n",
        "    print(f\"üîÑ Processing {file_in} to remove bad atoms...\")\n",
        "\n",
        "    cmd.load(filename=file_in, object=\"complex\")\n",
        "\n",
        "    # Remove specific elements\n",
        "    for element in [\"Mo\", \"B\", \"Li\", \"Xe\", \"As\", \"Cs\", \"V\", \"X\"]:\n",
        "        cmd.remove(f\"bymolecule elem {element}\")\n",
        "\n",
        "    # Save the cleaned file\n",
        "    cmd.save(filename=file_out, selection=\"complex\")\n",
        "\n",
        "    print(f\"‚úÖ Saved cleaned protein to: {file_out}\")\n",
        "\n",
        "# Run bad atom removal\n",
        "_remove_bad_atoms(protein_file_reduce, prot_pdb_rm_bad_path)\n",
        "\n",
        "print(\"‚úÖ Unwanted atoms removed using PyMOL!\")\n",
        "\n",
        "# Convert to PDBQT for docking\n",
        "prot_pdbqt_file_path = prot_pdb_rm_bad_path.replace(\"_reduce_rm_bad.pdb\", \"_reduce_rm_bad_input.pdbqt\")\n",
        "obabel_cmd = f\"obabel {prot_pdb_rm_bad_path} -xr -O {prot_pdbqt_file_path} -p 7.4 > /dev/null 2>&1\"\n",
        "subprocess.run(obabel_cmd, shell=True, check=True)\n",
        "print(\"‚úÖ Protein PDBQT file generated for docking!\")"
      ],
      "metadata": {
        "id": "Tqh4TSx1DzGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üñºÔ∏èüé®üîç Your protein (ab5_ckfix.pdb in this case) with Majority-voted pockets & their labels** { display-mode: \"form\" }\n",
        "\n",
        "import py3Dmol\n",
        "import numpy as np\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "# Get user input for residues (comma-separated)\n",
        "residues_input = \"86, 155, 181, 213, 216, 227, 255, 272, 330\"  # @param {type:\"string\"}\n",
        "\n",
        "# Option to show pocket labels (True to show, False to hide)\n",
        "show_pocket_labels = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Convert input to a list of integers, ignoring invalid entries\n",
        "residues_to_highlight = [int(res.strip()) for res in residues_input.split(\",\") if res.strip().isdigit()]\n",
        "highlight_color = \"red\"\n",
        "\n",
        "# Find all pocket files in WRK_DIR that match \"pocket...Majority...pdb\"\n",
        "cyan_pocket_paths = [\n",
        "    os.path.join(WRK_DIR, file) for file in os.listdir(WRK_DIR)\n",
        "    if file.startswith(\"pocket\") and \"Majority\" in file and file.endswith(\".pdb\")\n",
        "]\n",
        "\n",
        "# Function to compute the geometric center of a residue\n",
        "def compute_residue_center(residue):\n",
        "    atom_coords = [atom.coord for atom in residue if atom.element != 'H']\n",
        "    if atom_coords:\n",
        "        return np.mean(atom_coords, axis=0)\n",
        "    return None\n",
        "\n",
        "# Function to compute the geometric center of an entire pocket\n",
        "def compute_pocket_center(pocket_path):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"pocket\", pocket_path)\n",
        "    coords = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                for atom in residue:\n",
        "                    if atom.element != 'H':\n",
        "                        coords.append(atom.coord)\n",
        "    if coords:\n",
        "        return np.mean(coords, axis=0)\n",
        "    return None\n",
        "\n",
        "# Parse the main protein\n",
        "parser = PDBParser(QUIET=True)\n",
        "structure = parser.get_structure(\"protein\", protein_file)\n",
        "\n",
        "residue_centers = {}\n",
        "for model in structure:\n",
        "    for chain in model:\n",
        "        for residue in chain:\n",
        "            res_id = residue.id[1]\n",
        "            chain_id = chain.id\n",
        "            center = compute_residue_center(residue)\n",
        "            if center is not None:\n",
        "                residue_centers[(chain_id, res_id)] = center\n",
        "\n",
        "viewer = py3Dmol.view(width=1500, height=900)\n",
        "\n",
        "with open(protein_file, 'r') as protein_data:\n",
        "    viewer.addModel(protein_data.read(), 'pdb')\n",
        "viewer.setStyle({'model': 0}, {\"cartoon\": {\"color\": \"spectrum\"}})\n",
        "\n",
        "model_index = 1\n",
        "\n",
        "for pocket_path in cyan_pocket_paths:\n",
        "    with open(pocket_path, 'r') as pocket_data:\n",
        "        viewer.addModel(pocket_data.read(), 'pdb')\n",
        "\n",
        "    viewer.setStyle(\n",
        "        {'model': model_index},\n",
        "        {'stick': {'color': 'cyan', 'radius': 0.1, 'opacity': 0.8}}\n",
        "    )\n",
        "\n",
        "    # Compute & label pocket center if the user opted to show labels\n",
        "    if show_pocket_labels:\n",
        "        pocket_center = compute_pocket_center(pocket_path)\n",
        "        if pocket_center is not None:\n",
        "            x, y, z = map(float, pocket_center)\n",
        "            pocket_name = os.path.splitext(os.path.basename(pocket_path))[0]  # e.g. \"pocket_thr050_Minimal0\"\n",
        "\n",
        "            # Add label\n",
        "            viewer.addLabel(\n",
        "                pocket_name,\n",
        "                {\n",
        "                    \"position\": {\"x\": x, \"y\": y, \"z\": z},\n",
        "                    \"fontColor\": \"black\",\n",
        "                    \"font\": \"Helvetica\",\n",
        "                    \"fontSize\": 14,\n",
        "                    \"backgroundColor\": \"white\",\n",
        "                    \"showBackground\": True,\n",
        "                    \"opacity\": 1.0,\n",
        "                    \"inFront\": True,\n",
        "                    \"screenOffset\": {\"x\": 10, \"y\": -10}\n",
        "                }\n",
        "            )\n",
        "\n",
        "    model_index += 1\n",
        "\n",
        "# Highlight selected residues\n",
        "for (chain_id, res_id), center in residue_centers.items():\n",
        "    if res_id in residues_to_highlight:\n",
        "        viewer.addStyle(\n",
        "            {\"chain\": chain_id, \"resi\": res_id},\n",
        "            {\"stick\": {\"color\": highlight_color, \"radius\": 0.3}}\n",
        "        )\n",
        "        res_name = structure[0][chain_id][(' ', res_id, ' ')].resname\n",
        "        label = f\"{res_name}{res_id}\"\n",
        "        x, y, z = map(float, center)\n",
        "        viewer.addLabel(label, {\n",
        "            \"position\": {\"x\": x, \"y\": y, \"z\": z},\n",
        "            \"fontColor\": highlight_color,\n",
        "            \"font\": \"Helvetica\",\n",
        "            \"fontSize\": 14,\n",
        "            \"backgroundColor\": \"black\",\n",
        "            \"showBackground\": True,\n",
        "            \"opacity\": 1.0,\n",
        "            \"inFront\": True,\n",
        "            \"screenOffset\": {\"x\": 5, \"y\": -5}\n",
        "        })\n",
        "\n",
        "viewer.zoomTo()\n",
        "viewer.show()\n"
      ],
      "metadata": {
        "id": "TVxOKEByDzII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üñºÔ∏èüé®üîç Your protein (ab5_ckfix.pdb in this case) with Minimally-reported pockets & their labels** { display-mode: \"form\" }\n",
        "\n",
        "import py3Dmol\n",
        "import numpy as np\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "# Get user input for residues (comma-separated)\n",
        "residues_input = \"86, 155, 181, 213, 216, 227, 255, 272, 330\"  # @param {type:\"string\"}\n",
        "\n",
        "# Option to show pocket labels (True to show, False to hide)\n",
        "show_pocket_labels = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Convert input to a list of integers, ignoring invalid entries\n",
        "residues_to_highlight = [int(res.strip()) for res in residues_input.split(\",\") if res.strip().isdigit()]\n",
        "highlight_color = \"red\"\n",
        "\n",
        "# Find all pocket files in WRK_DIR that match \"pocket...Majority...pdb\"\n",
        "cyan_pocket_paths = [\n",
        "    os.path.join(WRK_DIR, file) for file in os.listdir(WRK_DIR)\n",
        "    if file.startswith(\"pocket\") and \"Minimal\" in file and file.endswith(\".pdb\")\n",
        "]\n",
        "\n",
        "# Function to compute the geometric center of a residue\n",
        "def compute_residue_center(residue):\n",
        "    atom_coords = [atom.coord for atom in residue if atom.element != 'H']\n",
        "    if atom_coords:\n",
        "        return np.mean(atom_coords, axis=0)\n",
        "    return None\n",
        "\n",
        "# Function to compute the geometric center of an entire pocket\n",
        "def compute_pocket_center(pocket_path):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"pocket\", pocket_path)\n",
        "    coords = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                for atom in residue:\n",
        "                    if atom.element != 'H':\n",
        "                        coords.append(atom.coord)\n",
        "    if coords:\n",
        "        return np.mean(coords, axis=0)\n",
        "    return None\n",
        "\n",
        "# Parse the main protein\n",
        "parser = PDBParser(QUIET=True)\n",
        "structure = parser.get_structure(\"protein\", protein_file)\n",
        "\n",
        "residue_centers = {}\n",
        "for model in structure:\n",
        "    for chain in model:\n",
        "        for residue in chain:\n",
        "            res_id = residue.id[1]\n",
        "            chain_id = chain.id\n",
        "            center = compute_residue_center(residue)\n",
        "            if center is not None:\n",
        "                residue_centers[(chain_id, res_id)] = center\n",
        "\n",
        "viewer = py3Dmol.view(width=1500, height=900)\n",
        "\n",
        "with open(protein_file, 'r') as protein_data:\n",
        "    viewer.addModel(protein_data.read(), 'pdb')\n",
        "viewer.setStyle({'model': 0}, {\"cartoon\": {\"color\": \"spectrum\"}})\n",
        "\n",
        "model_index = 1\n",
        "\n",
        "for pocket_path in cyan_pocket_paths:\n",
        "    with open(pocket_path, 'r') as pocket_data:\n",
        "        viewer.addModel(pocket_data.read(), 'pdb')\n",
        "\n",
        "    viewer.setStyle(\n",
        "        {'model': model_index},\n",
        "        {'stick': {'color': 'purple', 'radius': 0.3, 'opacity': 0.8}}\n",
        "    )\n",
        "\n",
        "    # Compute & label pocket center if the user opted to show labels\n",
        "    if show_pocket_labels:\n",
        "        pocket_center = compute_pocket_center(pocket_path)\n",
        "        if pocket_center is not None:\n",
        "            x, y, z = map(float, pocket_center)\n",
        "            pocket_name = os.path.splitext(os.path.basename(pocket_path))[0]  # e.g. \"pocket_thr050_Minimal0\"\n",
        "\n",
        "            # Add label\n",
        "            viewer.addLabel(\n",
        "                pocket_name,\n",
        "                {\n",
        "                    \"position\": {\"x\": x, \"y\": y, \"z\": z},\n",
        "                    \"fontColor\": \"black\",\n",
        "                    \"font\": \"Helvetica\",\n",
        "                    \"fontSize\": 14,\n",
        "                    \"backgroundColor\": \"white\",\n",
        "                    \"showBackground\": True,\n",
        "                    \"opacity\": 1.0,\n",
        "                    \"inFront\": True,\n",
        "                    \"screenOffset\": {\"x\": 10, \"y\": -10}\n",
        "                }\n",
        "            )\n",
        "\n",
        "    model_index += 1\n",
        "\n",
        "# Highlight selected residues\n",
        "for (chain_id, res_id), center in residue_centers.items():\n",
        "    if res_id in residues_to_highlight:\n",
        "        viewer.addStyle(\n",
        "            {\"chain\": chain_id, \"resi\": res_id},\n",
        "            {\"stick\": {\"color\": highlight_color, \"radius\": 0.3}}\n",
        "        )\n",
        "        res_name = structure[0][chain_id][(' ', res_id, ' ')].resname\n",
        "        label = f\"{res_name}{res_id}\"\n",
        "        x, y, z = map(float, center)\n",
        "        viewer.addLabel(label, {\n",
        "            \"position\": {\"x\": x, \"y\": y, \"z\": z},\n",
        "            \"fontColor\": highlight_color,\n",
        "            \"font\": \"Helvetica\",\n",
        "            \"fontSize\": 14,\n",
        "            \"backgroundColor\": \"black\",\n",
        "            \"showBackground\": True,\n",
        "            \"opacity\": 1.0,\n",
        "            \"inFront\": True,\n",
        "            \"screenOffset\": {\"x\": 5, \"y\": -5}\n",
        "        })\n",
        "\n",
        "viewer.zoomTo()\n",
        "viewer.show()\n"
      ],
      "metadata": {
        "id": "A9drYA68EENX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Use üñºÔ∏èüé®üîç LaboDock for illustration** { display-mode: \"form\" }\n",
        "\n",
        "# This function was taken from here: https://github.com/RyanZR/labodock\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Grid Box Calculation Methods\n",
        "\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import py3Dmol  # for the LaboSpace class usage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you need RDKit, RMSD, etc., make sure these are installed/imported:\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Draw\n",
        "# from rdkit.Chem import rdFMCS\n",
        "# import rmsd\n",
        "\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "\n",
        "############################\n",
        "#        GridBox Class\n",
        "############################\n",
        "\n",
        "class GridBox:\n",
        "\n",
        "    ranges = tuple[list[float], list[float], list[float]]\n",
        "    coords = tuple[float, float, float]\n",
        "    center_bxsize = tuple[tuple[float, float, float], tuple[float, float, float]]\n",
        "\n",
        "    def __init__(self, inpt_file: str) -> None:\n",
        "        self.inpt = open(inpt_file, 'r')\n",
        "        self.data = self.inpt.read()\n",
        "        self.inpt.close()\n",
        "        # If using RDKit, you need:\n",
        "        # self.cmol = Chem.MolFromPDBBlock(self.data)\n",
        "        # self.conf = self.cmol.GetConformer()\n",
        "        # self.ntom = self.cmol.GetNumAtoms()\n",
        "\n",
        "    def update_gridbox(self, mol_block: str) -> None:\n",
        "        # If using RDKit:\n",
        "        # self.cmol = Chem.MolFromPDBBlock(mol_block)\n",
        "        # self.conf = self.cmol.GetConformer()\n",
        "        # self.ntom = self.cmol.GetNumAtoms()\n",
        "        pass\n",
        "\n",
        "    def compute_coords(self) -> ranges:\n",
        "        # If using RDKit, example:\n",
        "        # x_coord = [self.conf.GetAtomPosition(c).x for c in range(self.ntom)]\n",
        "        # y_coord = [self.conf.GetAtomPosition(c).y for c in range(self.ntom)]\n",
        "        # z_coord = [self.conf.GetAtomPosition(c).z for c in range(self.ntom)]\n",
        "        # return x_coord, y_coord, z_coord\n",
        "        return [], [], []\n",
        "\n",
        "    def compute_ranges(self) -> ranges:\n",
        "        x, y, z = self.compute_coords()\n",
        "        x_range = [min(x), max(x)] if x else [0, 0]\n",
        "        y_range = [min(y), max(y)] if y else [0, 0]\n",
        "        z_range = [min(z), max(z)] if z else [0, 0]\n",
        "        return x_range, y_range, z_range\n",
        "\n",
        "    def compute_center(self, use_range: bool = True) -> coords:\n",
        "        if use_range:\n",
        "            x, y, z = self.compute_ranges()\n",
        "            x_center = round(np.mean(x), 3)\n",
        "            y_center = round(np.mean(y), 3)\n",
        "            z_center = round(np.mean(z), 3)\n",
        "            return x_center, y_center, z_center\n",
        "        else:\n",
        "            # If not using range, you'd just compute from coords\n",
        "            x, y, z = self.compute_coords()\n",
        "            if x:\n",
        "                x_center = round(np.mean(x), 3)\n",
        "                y_center = round(np.mean(y), 3)\n",
        "                z_center = round(np.mean(z), 3)\n",
        "                return x_center, y_center, z_center\n",
        "            else:\n",
        "                return (0.0, 0.0, 0.0)\n",
        "\n",
        "    def generate_res_molblock(self, residues_list: list[str]) -> str:\n",
        "        res_lines = [line for line in self.data.split('\\n')\n",
        "                     if line[22:26].lstrip() in residues_list\n",
        "                     and 'END' not in line]\n",
        "        res_block = '\\n'.join(res_lines)\n",
        "        return res_block\n",
        "\n",
        "    def labox(self, scale: float = 2.0) -> coords:\n",
        "        xr, yr, zr = self.compute_ranges()\n",
        "        center = self.compute_center()\n",
        "        bxsize = (round(abs(xr[0] - xr[1]) * scale, 3),\n",
        "                  round(abs(yr[0] - yr[1]) * scale, 3),\n",
        "                  round(abs(zr[0] - zr[1]) * scale, 3))\n",
        "        return center, bxsize\n",
        "\n",
        "    def eboxsize(self, gy_box_ratio: float = 0.23, modified: bool = False) -> center_bxsize:\n",
        "        xc, yc, zc = self.compute_coords()\n",
        "        center = self.compute_center(modified)\n",
        "        if xc:\n",
        "            distsq = [(x - center[0])**2 + (y - center[1])**2 + (z - center[2])**2\n",
        "                      for x, y, z in zip(xc, yc, zc)]\n",
        "            bxsize = (round(np.sqrt(sum(distsq) / len(xc)) / gy_box_ratio, 3),) * 3\n",
        "            return center, bxsize\n",
        "        else:\n",
        "            return (0.0, 0.0, 0.0), (0.0, 0.0, 0.0)\n",
        "\n",
        "    def autodock_grid(self) -> center_bxsize:\n",
        "        xr, yr, zr = self.compute_ranges()\n",
        "        center = self.compute_center()\n",
        "        bxsize = (22.5, 22.5, 22.5)\n",
        "        return center, bxsize\n",
        "\n",
        "    def defined_by_res(self, residue_number: str, scale: float = 1.25) -> center_bxsize:\n",
        "        res_list = residue_number.replace(',', ' ').split()\n",
        "        res_block = self.generate_res_molblock(res_list)\n",
        "        self.update_gridbox(res_block)\n",
        "        return self.labox(scale=scale)\n",
        "\n",
        "\n",
        "############################\n",
        "#    ComputeRMSD Class\n",
        "############################\n",
        "\n",
        "class ComputeRMSD:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.MCS_mol = None\n",
        "        self.MCS_png = None\n",
        "\n",
        "    def load_molecule(self, inpt_file: str, remove_Hs: bool = True) -> tuple:\n",
        "        # Example placeholder\n",
        "        # molecule = io.loadmol(inpt_file)\n",
        "        # if remove_Hs:\n",
        "        #     molecule.strip()\n",
        "        # ...\n",
        "        return (), (), (), (), None\n",
        "\n",
        "    def mol_to_png(self, mol: object) -> object:\n",
        "        # Example placeholder\n",
        "        # legend = 'Maximum Common Substructure'\n",
        "        # png = Draw.MolToImage(mol, legend=legend)\n",
        "        # return png\n",
        "        return None\n",
        "\n",
        "    def find_MCS(self, ref: tuple, lig: tuple) -> object:\n",
        "        if self.MCS_mol is None:\n",
        "            # MCS_obj = rdFMCS.FindMCS([ref[4], lig[4]])\n",
        "            # MCS_mol = Chem.MolFromSmarts(MCS_obj.smartsString)\n",
        "            # MCS_png = self.mol_to_png(MCS_mol)\n",
        "            # self.MCS_mol = MCS_mol\n",
        "            # self.MCS_png = MCS_png\n",
        "            pass\n",
        "        return self.MCS_mol\n",
        "\n",
        "    def hung_RMSD(self, ref: tuple, lig: tuple) -> float:\n",
        "        try:\n",
        "            # hRMSD = round(rmsd.hrmsd(ref[1], lig[1], ref[2], lig[2]), 3)\n",
        "            hRMSD = 0.0\n",
        "        except:\n",
        "            hRMSD = 'ERROR'\n",
        "        return hRMSD\n",
        "\n",
        "    def symm_RMSD(self, ref: tuple, lig: tuple, minimise: bool = False) -> float:\n",
        "        try:\n",
        "            # sRMSD = round(rmsd.symmrmsd(...), 3)\n",
        "            sRMSD = 0.0\n",
        "        except:\n",
        "            sRMSD = 'ERROR'\n",
        "        return sRMSD\n",
        "\n",
        "    def labo_RMSD(self, ref: tuple, lig: tuple) -> float:\n",
        "        # Example placeholder\n",
        "        return 0.0\n",
        "\n",
        "    def rmsd_report(self,\n",
        "                    ref: tuple,\n",
        "                    lig: tuple,\n",
        "                    lRMSD: bool = True,\n",
        "                    hRMSD: bool = True,\n",
        "                    sRMSD: bool = True\n",
        "                    ) -> dict:\n",
        "        report = {}\n",
        "        report['NAME'] = [lig[0] if len(lig) > 0 else 'N/A']\n",
        "        if lRMSD:\n",
        "            report['LABO_RMSD'] = [self.labo_RMSD(ref, lig)]\n",
        "        if hRMSD:\n",
        "            report['HUNG_RMSD'] = [self.hung_RMSD(ref, lig)]\n",
        "        if sRMSD:\n",
        "            report['SYMM_RMSD'] = [self.symm_RMSD(ref, lig)]\n",
        "        return report\n",
        "\n",
        "\n",
        "############################\n",
        "#   Constants & Dictionaries\n",
        "############################\n",
        "\n",
        "AA_HB = {\n",
        "    'ALA': 1.8, 'ARG': -4.5, 'ASN': -3.5, 'ASP': -3.5, 'CYS': 2.5,\n",
        "    'GLN': -3.5, 'GLU': -3.5, 'GLY': -0.4, 'HIS': -3.2, 'ILE': 4.5,\n",
        "    'LEU': 3.8, 'LYS': -3.9, 'MET': 1.9, 'PHE': 2.8, 'PRO': -1.6,\n",
        "    'SER': -0.8, 'THR': -0.7, 'TRP': -0.9, 'TYR': -1.3, 'VAL': 4.2\n",
        "}\n",
        "\n",
        "AA_PI = {\n",
        "    'ALA': 6.0, 'ARG': 10.76, 'ASN': 5.41, 'ASP': 2.77, 'CYS': 5.07,\n",
        "    'GLN': 5.65, 'GLU': 3.22, 'GLY': 5.97, 'HIS': 7.59, 'ILE': 6.02,\n",
        "    'LEU': 5.98, 'LYS': 9.74, 'MET': 5.74, 'PHE': 5.48, 'PRO': 6.3,\n",
        "    'SEC': 5.68, 'SER': 5.68, 'THR': 5.6, 'TRP': 5.89, 'TYR': 5.66,\n",
        "    'VAL': 5.96\n",
        "}\n",
        "\n",
        "BOND_COL = {\n",
        "    'HYDROPHOBIC': ['0x59e382', 'GREEN'],\n",
        "    'HBOND': ['0x59bee3', 'LIGHT BLUE'],\n",
        "    'WATERBRIDGE': ['0x4c4cff', 'BLUE'],\n",
        "    'SALTBRIDGE': ['0xefd033', 'YELLOW'],\n",
        "    'PISTACKING': ['0xb559e3', 'PURPLE'],\n",
        "    'PICATION': ['0xe359d8', 'VIOLET'],\n",
        "    'HALOGEN': ['0x59bee3', 'LIGHT BLUE'],\n",
        "    'METAL': ['0xe35959', 'ORANGE']\n",
        "}\n",
        "\n",
        "\n",
        "############################\n",
        "#     Colour Gradients\n",
        "############################\n",
        "\n",
        "def sequential_gradient(value: float,\n",
        "                        min_value: float,\n",
        "                        max_value: float,\n",
        "                        targ_colour: str = '00ff00',\n",
        "                        interpolation: float = 0.0\n",
        "                        ) -> str:\n",
        "    norm_val = (value - min_value) / (max_value - min_value)\n",
        "    rgb = tuple(int(targ_colour[d:d+2], 16) for d in (0, 2, 4))\n",
        "    r = int(255 - (255 - rgb[0]) * (1 - interpolation) * norm_val)\n",
        "    g = int(255 - (255 - rgb[1]) * (1 - interpolation) * norm_val)\n",
        "    b = int(255 - (255 - rgb[2]) * (1 - interpolation) * norm_val)\n",
        "    hex_code = f'#{r:02x}{g:02x}{b:02x}'\n",
        "    return hex_code\n",
        "\n",
        "def diverging_gradient(value: float,\n",
        "                       min_value: float,\n",
        "                       max_value: float,\n",
        "                       base_colour: str = 'ff0000',\n",
        "                       targ_colour: str = '0000ff',\n",
        "                       interpolation: float = 0.3\n",
        "                       ) -> str:\n",
        "    norm_val = (value - min_value) / (max_value - min_value)\n",
        "    white = (255, 255, 255)\n",
        "    rgb_A = tuple(int(base_colour[d:d+2], 16) for d in (0, 2, 4))\n",
        "    rgb_B = tuple(int(targ_colour[d:d+2], 16) for d in (0, 2, 4))\n",
        "\n",
        "    if norm_val < 0.5 - interpolation / 2:\n",
        "        factor = norm_val / (0.5 - interpolation / 2)\n",
        "        r = int(rgb_A[0] + (white[0] - rgb_A[0]) * factor)\n",
        "        g = int(rgb_A[1] + (white[1] - rgb_A[1]) * factor)\n",
        "        b = int(rgb_A[2] + (white[2] - rgb_A[2]) * factor)\n",
        "    elif norm_val > 0.5 + interpolation / 2:\n",
        "        factor = (norm_val - 0.5 - interpolation / 2) / (0.5 - interpolation / 2)\n",
        "        r = int(white[0] + (rgb_B[0] - white[0]) * factor)\n",
        "        g = int(white[1] + (rgb_B[1] - white[1]) * factor)\n",
        "        b = int(white[2] + (rgb_B[2] - white[2]) * factor)\n",
        "    else:\n",
        "        r, g, b = white\n",
        "\n",
        "    hex_code = f'#{r:02x}{g:02x}{b:02x}'\n",
        "    return hex_code\n",
        "\n",
        "def a2c_converter(aa_map: dict, grad_func) -> dict:\n",
        "    min_value = min(aa_map.values())\n",
        "    max_value = max(aa_map.values())\n",
        "    aa_dict = {aa: grad_func(value, min_value, max_value)\n",
        "               for aa, value in aa_map.items()}\n",
        "    return aa_dict\n",
        "\n",
        "\n",
        "############################\n",
        "#   Built-in Styling\n",
        "############################\n",
        "\n",
        "def builtin_style(style: str, opacity: float = 1.0) -> dict:\n",
        "    # Basic examples\n",
        "    if any(kw in style for kw in ('Carbon', 'chain', 'ssJmol', 'ssPyMol')):\n",
        "        style_dict = {'colorscheme': style}\n",
        "    elif style == 'hydrophobicity':\n",
        "        style_dict = {'colorscheme': {\n",
        "            'prop': 'resn',\n",
        "            'map': a2c_converter(AA_HB, sequential_gradient)}}\n",
        "    elif style == 'isoelectric points':\n",
        "        style_dict = {'colorscheme': {\n",
        "            'prop': 'resn',\n",
        "            'map': a2c_converter(AA_PI, diverging_gradient)}}\n",
        "    elif style == 'b factor':\n",
        "        style_dict = {'colorscheme': {\n",
        "            'prop': 'b', 'gradient': 'rwb', 'min': 90, 'max': 50}}\n",
        "    else:\n",
        "        style_dict = {'color': style}\n",
        "\n",
        "    style_dict.update({'opacity': opacity, 'singleBonds': False})\n",
        "    return style_dict\n",
        "\n",
        "\n",
        "############################\n",
        "#    Colour Scale\n",
        "############################\n",
        "\n",
        "def colour_scale(aa_map: dict, grad_func) -> None:\n",
        "    min_value = min(aa_map.values())\n",
        "    max_value = max(aa_map.values())\n",
        "    linear_values = np.linspace(min_value, max_value, 100)\n",
        "    colours = [grad_func(value, min_value, max_value)\n",
        "               for value in linear_values]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4.85, 0.25))\n",
        "    norm_value = plt.Normalize(min_value, max_value)\n",
        "    colour_map = plt.cm.colors.ListedColormap(colours)\n",
        "    scalar_map = plt.cm.ScalarMappable(norm_value, colour_map)\n",
        "    scalar_map.set_array([])\n",
        "\n",
        "    cscale = plt.colorbar(scalar_map, ax, orientation='horizontal')\n",
        "    cscale.set_ticks([min_value, max_value])\n",
        "\n",
        "def show_cscale(rept_info: dict, surf_info: dict) -> None:\n",
        "    def cs_selector() -> str:\n",
        "        if any(surf_info):\n",
        "            style = [*surf_info.values()][0]\n",
        "        elif any(rept_info):\n",
        "            style = [*rept_info.values()][0]\n",
        "        else:\n",
        "            style = None\n",
        "        return style\n",
        "\n",
        "    def cs_display(style: str):\n",
        "        if style == 'hydrophobicity':\n",
        "            label_title(style, 'Less', 'More')\n",
        "            colour_scale(AA_HB, sequential_gradient)\n",
        "        elif style == 'isoelectric points':\n",
        "            label_title(style, 'Acid', 'Base')\n",
        "            colour_scale(AA_PI, diverging_gradient)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def label_title(text: str, min: str, max: str) -> None:\n",
        "        print('-' * 55)\n",
        "        print(f'{min}{text.upper():^47}{max}')\n",
        "        print('-' * 55)\n",
        "\n",
        "    cs_display(cs_selector())\n",
        "\n",
        "\n",
        "############################\n",
        "#   Extract Box Config\n",
        "############################\n",
        "\n",
        "def extract_config(inpt_file: str) -> tuple:\n",
        "    with open(inpt_file, 'r') as inpt:\n",
        "        data = [line.split() for line in inpt.readlines()]\n",
        "    center = (float(data[0][2]), float(data[1][2]), float(data[2][2]))\n",
        "    bxsize = (float(data[4][2]), float(data[5][2]), float(data[6][2]))\n",
        "    return center, bxsize\n",
        "\n",
        "\n",
        "############################\n",
        "#   Dist Unification\n",
        "############################\n",
        "\n",
        "def unify_distance_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates/overwrites a single 'DISTANCE' column for all rows.\n",
        "    For hydrogen bonds, we pull from 'DIST_H-A' or 'DIST_D-A' if 'DIST' is NaN.\n",
        "    \"\"\"\n",
        "    def get_distance(row):\n",
        "        dist_val = row.get('DIST', np.nan)\n",
        "        # If 'DIST' is NaN and it's an HBOND, check 'DIST_H-A' or 'DIST_D-A'\n",
        "        if pd.isna(dist_val) and row.get('BOND') == 'HBOND':\n",
        "            dist_val = row.get('DIST_H-A', np.nan)\n",
        "            if pd.isna(dist_val):\n",
        "                dist_val = row.get('DIST_D-A', np.nan)\n",
        "        return dist_val\n",
        "\n",
        "    df['DISTANCE'] = df.apply(get_distance, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "############################\n",
        "#   Interaction Dictionary\n",
        "############################\n",
        "\n",
        "def interaction_dict(inpt_file: str,\n",
        "                     interactions: str = '',\n",
        "                     usage: str = 'view' or 'lbsp') -> dict:\n",
        "    usg_map = {'lbsp': 0, 'view': 1}\n",
        "\n",
        "    def filter_df(int_df: pd.DataFrame, interactions: list = []) -> pd.DataFrame:\n",
        "        return int_df[int_df['BOND'].isin(interactions)] if interactions else int_df\n",
        "\n",
        "    def s2f_dict(item: dict) -> dict:\n",
        "        \"\"\"Convert '(x, y, z)' string to a float tuple.\"\"\"\n",
        "        newdict = {}\n",
        "        for key, value in item.items():\n",
        "            if isinstance(value, str) and value.startswith('(') and value.endswith(')'):\n",
        "                # Remove parentheses, split by comma\n",
        "                coords = value[1:-1].split(',')\n",
        "                coords = [float(c.strip()) for c in coords]\n",
        "                newdict[key] = tuple(coords)\n",
        "            else:\n",
        "                newdict[key] = value\n",
        "        return newdict\n",
        "\n",
        "    def b2c_dict(item: dict) -> dict:\n",
        "        \"\"\"Map bond type -> color from BOND_COL.\"\"\"\n",
        "        newdict = {}\n",
        "        for key, val in item.items():\n",
        "            # If bond type not recognized, fallback to e.g. 'GREY'\n",
        "            if val in BOND_COL:\n",
        "                newdict[key] = BOND_COL[val][usg_map[usage]]\n",
        "            else:\n",
        "                newdict[key] = 'GREY'\n",
        "        return newdict\n",
        "\n",
        "    # 1) Read CSV\n",
        "    df = pd.read_csv(inpt_file)\n",
        "\n",
        "    # 2) Unify distances so we have a single 'DISTANCE' column\n",
        "    df = unify_distance_columns(df)\n",
        "\n",
        "    # 3) Filter by user interactions\n",
        "    intrxn = interactions.replace(',', ' ').split()\n",
        "    df = filter_df(df, intrxn)\n",
        "\n",
        "    # 4) Convert to dictionary\n",
        "    int_dict = df.to_dict()\n",
        "\n",
        "    # 5) Convert LIGCOO & PROTCOO from strings to float tuples\n",
        "    int_dict['LIGCOO'] = s2f_dict(int_dict['LIGCOO'])\n",
        "    int_dict['PROTCOO'] = s2f_dict(int_dict['PROTCOO'])\n",
        "\n",
        "    # 6) Convert BOND -> color\n",
        "    int_dict['COLOR'] = b2c_dict(int_dict['BOND'])\n",
        "\n",
        "    return int_dict\n",
        "\n",
        "\n",
        "############################\n",
        "#    Midpoint Helper\n",
        "############################\n",
        "\n",
        "def find_midpoint(coords: list) -> tuple[float, float, float]:\n",
        "    arr = np.array(coords)\n",
        "    mean_vals = np.mean(arr, axis=0)\n",
        "    return tuple(round(x, 3) for x in mean_vals)\n",
        "\n",
        "\n",
        "############################\n",
        "#    LaboSpace Viewer\n",
        "############################\n",
        "\n",
        "class LaboSpace:\n",
        "\n",
        "    residue_style = {\n",
        "        'stick': {'colorscheme': 'orangeCarbon', 'radius': 0.15}\n",
        "    }\n",
        "    residue_label = {\n",
        "        'alignment': 'bottomLeft',\n",
        "        'showBackground': False,\n",
        "        'inFront': True,\n",
        "        'fontSize': 14,\n",
        "        'fontColor': '0x000000',\n",
        "        'screenOffset': {'x': 25, 'y': 25}\n",
        "    }\n",
        "    atom_label = {\n",
        "        'alignment': 'bottomLeft',\n",
        "        'showBackground': False,\n",
        "        'inFront': True,\n",
        "        'fontSize': 14,\n",
        "        'fontColor': '0x000000',\n",
        "        'screenOffset': {'x': 10, 'y': 10}\n",
        "    }\n",
        "\n",
        "    def __init__(self, vw: int = 500, vh: int = 500) -> None:\n",
        "        self.mview = py3Dmol.view(width=vw, height=vh)\n",
        "        self.count = -1\n",
        "        self.residues = []\n",
        "\n",
        "    def read_moldata(self, inpt_file: str) -> str:\n",
        "        with open(inpt_file, 'r') as f:\n",
        "            data = f.read()\n",
        "        return data\n",
        "\n",
        "    def load_receptor(self, inpt_file: str) -> object:\n",
        "        data = self.read_moldata(inpt_file)\n",
        "        self.mview.addModel(data, 'pdb')\n",
        "        self.count += 1\n",
        "        return self\n",
        "\n",
        "    def load_ligand(self, inpt_file: str) -> object:\n",
        "        data = self.read_moldata(inpt_file)\n",
        "        self.mview.addModel(data, 'pdb')\n",
        "        self.count += 1\n",
        "        return self\n",
        "\n",
        "    def set_style(self,\n",
        "                  show_represent: bool = True,\n",
        "                  represent_type: str = 'cartoon',\n",
        "                  represent_style: dict = {}\n",
        "                  ) -> object:\n",
        "        if show_represent:\n",
        "            self.mview.setStyle(\n",
        "                {'model': self.count},\n",
        "                {represent_type: represent_style}\n",
        "            )\n",
        "        else:\n",
        "            self.mview.setStyle({'model': self.count}, {})\n",
        "        return self\n",
        "\n",
        "    def add_style(self,\n",
        "                  show_represent: bool = True,\n",
        "                  represent_style: dict = {}\n",
        "                  ) -> object:\n",
        "        if show_represent:\n",
        "            self.mview.addStyle({'model': self.count}, represent_style)\n",
        "        return self\n",
        "\n",
        "    def add_residues(self,\n",
        "                     show_residues: bool = True,\n",
        "                     residue_number: str = ''\n",
        "                     ) -> object:\n",
        "        if show_residues and residue_number:\n",
        "            res = residue_number.replace(',', ' ').split()\n",
        "            self.residues.extend(list(set(res)))\n",
        "            self.mview.addStyle(\n",
        "                {'and': [{'model': self.count}, {'resi': self.residues}]},\n",
        "                self.residue_style)\n",
        "            self.mview.addResLabels(\n",
        "                {'and': [{'model': self.count}, {'resi': self.residues}]},\n",
        "                self.residue_label)\n",
        "        return self\n",
        "\n",
        "    def add_surface(self,\n",
        "                    show_surface: bool = True,\n",
        "                    surface_type: str = 'SES',\n",
        "                    surface_style: dict = {}\n",
        "                    ) -> object:\n",
        "        if show_surface:\n",
        "            self.mview.addSurface(\n",
        "                surface_type,\n",
        "                surface_style,\n",
        "                {'model': self.count})\n",
        "        return self\n",
        "\n",
        "    def add_gridbox(self,\n",
        "                    show_gridbox: bool,\n",
        "                    center: tuple[float, float, float],\n",
        "                    bxsize: tuple[float, float, float]\n",
        "                    ) -> object:\n",
        "        if show_gridbox:\n",
        "            bxi, byi, bzi = center\n",
        "            bxf, byf, bzf = bxsize\n",
        "            self.mview.addBox({\n",
        "                'center': {'x': bxi, 'y': byi, 'z': bzi},\n",
        "                'dimensions': {'w': bxf, 'h': byf, 'd': bzf},\n",
        "                'color': 'skyBlue',\n",
        "                'opacity': 0.6\n",
        "            })\n",
        "            self.mview.addLabel(\n",
        "                f'center: {bxi:>8}, {byi:>8}, {bzi:>8}',\n",
        "                {'showBackground': False,\n",
        "                 'fontSize': 14,\n",
        "                 'fontColor': '0x000000',\n",
        "                 'useScreen': True,\n",
        "                 'screenOffset': {'x': 15, 'y': 0}})\n",
        "            self.mview.addLabel(\n",
        "                f'bxsize: {bxf:>8}, {byf:>8}, {bzf:>8}',\n",
        "                {'showBackground': False,\n",
        "                 'fontSize': 14,\n",
        "                 'fontColor': '0x000000',\n",
        "                 'useScreen': True,\n",
        "                 'screenOffset': {'x': 15, 'y': -20}})\n",
        "        return self\n",
        "\n",
        "    def add_interaction(self,\n",
        "                        interaction_file: str,\n",
        "                        show_interaction: bool = True,\n",
        "                        select_interaction: list = []\n",
        "                        ) -> object:\n",
        "        \"\"\"\n",
        "        Now reads from 'DISTANCE' instead of 'DIST'.\n",
        "        \"\"\"\n",
        "        if show_interaction:\n",
        "            int_dict = interaction_dict(interaction_file, select_interaction, 'lbsp')\n",
        "\n",
        "            # IMPORTANT: we now read 'DISTANCE' from the dictionary\n",
        "            dist = int_dict['DISTANCE'].values()\n",
        "            bond = int_dict['BOND'].values()\n",
        "            resn = int_dict['RESNR'].values()\n",
        "            ligcoo = int_dict['LIGCOO'].values()\n",
        "            prtcoo = int_dict['PROTCOO'].values()\n",
        "            color = int_dict['COLOR'].values()\n",
        "\n",
        "            # Add missing residues to the style\n",
        "            int_res = list(set(resn) - set(self.residues))\n",
        "            self.residues.extend(int_res)\n",
        "            self.mview.addStyle(\n",
        "                {'and': [{'model': 0}, {'resi': int_res}]},\n",
        "                self.residue_style)\n",
        "            self.mview.addResLabels(\n",
        "                {'and': [{'model': 0}, {'resi': int_res}]},\n",
        "                self.residue_label)\n",
        "\n",
        "            # Draw cylinders & distance labels\n",
        "            for dval, col, lig, prt in zip(dist, color, ligcoo, prtcoo):\n",
        "                if isinstance(lig, tuple) and isinstance(prt, tuple):\n",
        "                    mid = find_midpoint([lig, prt])\n",
        "                    self.mview.addCylinder({\n",
        "                        'start': {'x': lig[0], 'y': lig[1], 'z': lig[2]},\n",
        "                        'end': {'x': prt[0], 'y': prt[1], 'z': prt[2]},\n",
        "                        'radius': 0.05,\n",
        "                        'fromCap': 1,\n",
        "                        'toCap': 1,\n",
        "                        'color': col,\n",
        "                        'dashed': True\n",
        "                    })\n",
        "                    label_text = f\"{dval} √Ö\" if not pd.isna(dval) else \"N/A\"\n",
        "                    self.mview.addLabel(\n",
        "                        label_text,\n",
        "                        {'position': {'x': mid[0], 'y': mid[1], 'z': mid[2]},\n",
        "                         'alignment': 'bottomLeft',\n",
        "                         'inFront': False,\n",
        "                         'backgroundColor': col,\n",
        "                         'fontSize': 10,\n",
        "                         'screenOffset': {'x': 10, 'y': 10}})\n",
        "        return self\n",
        "\n",
        "    def label_atoms(self, show_label: bool = False) -> object:\n",
        "        if show_label:\n",
        "            self.mview.addPropertyLabels(\n",
        "                'atom',\n",
        "                {'model': self.count},\n",
        "                self.atom_label)\n",
        "        return self\n",
        "\n",
        "    def view_space(self,\n",
        "                   zoom_model: int = -1,\n",
        "                   slab_view: bool = False,\n",
        "                   slab_model: int = -1,\n",
        "                   background_colour: str = '0xFFFFFF'\n",
        "                   ) -> None:\n",
        "        self.mview.setBackgroundColor(background_colour)\n",
        "        self.mview.setProjection('orthographic')\n",
        "        self.mview.zoomTo({'model': zoom_model})\n",
        "        if slab_view:\n",
        "            self.mview.fitSlab({'model': slab_model})\n",
        "        self.mview.show()\n",
        "\n",
        "\n",
        "############################\n",
        "#  Pocket Box from PDB\n",
        "############################\n",
        "\n",
        "def compute_box_from_pocket(pocket_path: str, threshold: float):\n",
        "    \"\"\"\n",
        "    Reads the pocket file, parses all atom coordinates, computes the bounding box\n",
        "    (min and max coords), then inflates each dimension by 2*threshold.\n",
        "    Returns (center, box_dimensions).\n",
        "    \"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"pocket_struct\", pocket_path)\n",
        "\n",
        "    coords = []\n",
        "    for atom in structure.get_atoms():\n",
        "        coords.append(atom.coord)\n",
        "    coords = np.array(coords)\n",
        "\n",
        "    min_coords = np.min(coords, axis=0)\n",
        "    max_coords = np.max(coords, axis=0)\n",
        "\n",
        "    center = (max_coords + min_coords) / 2.0\n",
        "    size_x = (max_coords[0] - min_coords[0]) + 2 * threshold\n",
        "    size_y = (max_coords[1] - min_coords[1]) + 2 * threshold\n",
        "    size_z = (max_coords[2] - min_coords[2]) + 2 * threshold\n",
        "\n",
        "    return tuple(center), (size_x, size_y, size_z)\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# **User Inputs**\n",
        "protein_path = protein_file\n",
        "pocket_path  = \"/content/ABHD5/pocket_thr050_Majority0.pdb\"  # @param {type:\"string\"}\n",
        "residues_to_highlight = \"86, 155, 181, 213, 216, 227, 255, 272, 330\"  # @param {type:\"string\"}\n",
        "threshold = 2.0  # @param {type:\"number\"} Each dimension is expanded by +/- threshold √Ö\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Compute bounding box from pocket\n",
        "center, bxsize = compute_box_from_pocket(pocket_path, threshold)\n",
        "\n",
        "# ----- FIX: convert np.float32 or np.float64 -> plain Python float ------\n",
        "center = tuple(float(c) for c in center)   # ensures JSON serializable\n",
        "bxsize = tuple(float(s) for s in bxsize)   # ensures JSON serializable\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# Create the LaboSpace viewer\n",
        "LBSP = LaboSpace(960, 640)\n",
        "\n",
        "# Load and style the protein (white cartoon)\n",
        "LBSP.load_receptor(protein_path)\\\n",
        "    .set_style(\n",
        "        show_represent=True,\n",
        "        represent_type='cartoon',\n",
        "        represent_style={'color': 'white'}  # same color style as your snippet\n",
        "    )\\\n",
        "    .add_residues(\n",
        "        show_residues=True,\n",
        "        residue_number=residues_to_highlight\n",
        "    )\n",
        "\n",
        "# Load and style the pocket (cyan sticks, radius=0.3, opacity=1)\n",
        "LBSP.load_ligand(pocket_path)\\\n",
        "    .set_style(\n",
        "        show_represent=True,\n",
        "        represent_type='stick',\n",
        "        represent_style={'color': 'cyan', 'radius': 0.3, 'opacity': 1}\n",
        "    )\n",
        "\n",
        "# Add a semi-transparent skyBlue bounding box\n",
        "LBSP.add_gridbox(\n",
        "    show_gridbox=True,\n",
        "    center=center,\n",
        "    bxsize=bxsize\n",
        ")\n",
        "\n",
        "# Add solid black edges around the bounding box\n",
        "bxi, byi, bzi = center\n",
        "bxf, byf, bzf = bxsize\n",
        "\n",
        "edges = [\n",
        "    # Bottom face\n",
        "    ((bxi - bxf/2, byi - byf/2, bzi - bzf/2), (bxi + bxf/2, byi - byf/2, bzi - bzf/2)),\n",
        "    ((bxi - bxf/2, byi - byf/2, bzi - bzf/2), (bxi - bxf/2, byi + byf/2, bzi - bzf/2)),\n",
        "    ((bxi + bxf/2, byi + byf/2, bzi - bzf/2), (bxi + bxf/2, byi - byf/2, bzi - bzf/2)),\n",
        "    ((bxi + bxf/2, byi + byf/2, bzi - bzf/2), (bxi - bxf/2, byi + byf/2, bzi - bzf/2)),\n",
        "\n",
        "    # Top face\n",
        "    ((bxi - bxf/2, byi - byf/2, bzi + bzf/2), (bxi + bxf/2, byi - byf/2, bzi + bzf/2)),\n",
        "    ((bxi - bxf/2, byi - byf/2, bzi + bzf/2), (bxi - bxf/2, byi + byf/2, bzi + bzf/2)),\n",
        "    ((bxi + bxf/2, byi + byf/2, bzi + bzf/2), (bxi + bxf/2, byi - byf/2, bzi + bzf/2)),\n",
        "    ((bxi + bxf/2, byi + byf/2, bzi + bzf/2), (bxi - bxf/2, byi + byf/2, bzi + bzf/2)),\n",
        "\n",
        "    # Vertical edges connecting top & bottom\n",
        "    ((bxi - bxf/2, byi - byf/2, bzi - bzf/2), (bxi - bxf/2, byi - byf/2, bzi + bzf/2)),\n",
        "    ((bxi + bxf/2, byi - byf/2, bzi - bzf/2), (bxi + bxf/2, byi - byf/2, bzi + bzf/2)),\n",
        "    ((bxi - bxf/2, byi + byf/2, bzi - bzf/2), (bxi - bxf/2, byi + byf/2, bzi + bzf/2)),\n",
        "    ((bxi + bxf/2, byi + byf/2, bzi - bzf/2), (bxi + bxf/2, byi + byf/2, bzi + bzf/2))\n",
        "]\n",
        "\n",
        "for start, end in edges:\n",
        "    LBSP.mview.addCylinder({\n",
        "        'start': {'x': start[0], 'y': start[1], 'z': start[2]},\n",
        "        'end':   {'x': end[0],   'y': end[1],   'z': end[2]},\n",
        "        'radius': 0.05,\n",
        "        'color': 'black',\n",
        "        'opacity': 1.0\n",
        "    })\n",
        "\n",
        "# Display the final result\n",
        "LBSP.view_space(\n",
        "    zoom_model=-1,\n",
        "    slab_view=False,\n",
        "    slab_model=-1,\n",
        "    background_colour='0xFFFFFF'\n",
        ")\n"
      ],
      "metadata": {
        "id": "_PwaJB8lEESl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üß™ (Optional): Upload Initial Ligand Conformation File, if you have it. ‚öõÔ∏è** { display-mode: \"form\" }\n",
        "# @markdown If you don't have it, skip this step.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from rdkit.Chem import MolFromMolFile, AddHs\n",
        "from meeko import MoleculePreparation\n",
        "\n",
        "# Upload ligand file\n",
        "print(\"\\nüì§ Please upload your ligand file (SDF format):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ensure only one file is uploaded\n",
        "lig_sdf_filename = list(uploaded.keys())[0]\n",
        "uploaded_file_path = os.path.join(\"/content\", lig_sdf_filename)  # File initially goes to /content/\n",
        "\n",
        "# Move ligand file to the working directory\n",
        "LIG_sdf_lFile = os.path.join(WRK_DIR, lig_sdf_filename)\n",
        "shutil.move(uploaded_file_path, LIG_sdf_lFile)  # Move file\n",
        "print(f\"‚úÖ Ligand moved to working directory: `{LIG_sdf_lFile}`\")\n",
        "\n",
        "# Validate file existence\n",
        "assert os.path.exists(LIG_sdf_lFile), f\"‚ùå File not found: {LIG_sdf_lFile}\"\n",
        "\n",
        "print(\"\\nüîÑ Preparing ligand for docking...\")\n",
        "\n",
        "try:\n",
        "    # Load ligand using RDKit\n",
        "    lig = MolFromMolFile(LIG_sdf_lFile, sanitize=True)\n",
        "    if lig is None:\n",
        "        raise ValueError(f'Failed to load ligand from {LIG_sdf_lFile}')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Error loading ligand: {e}')\n",
        "    print('‚ö†Ô∏è Please ensure the ligand file is a valid SDF format and try again.')\n",
        "    raise SystemExit\n",
        "\n",
        "# Ensure ligand has at least one conformer\n",
        "lig = AddHs(lig, addCoords=True)\n",
        "assert lig.GetNumConformers() >= 1, \"‚ùå Ligand must have at least one conformer!\"\n",
        "\n",
        "# Prepare ligand using Meeko\n",
        "meeko_prep = MoleculePreparation()\n",
        "meeko_prep.prepare(lig)\n",
        "\n",
        "# Define output path in WRK_DIR and save the prepared ligand\n",
        "lig_pdbqt_file_path = os.path.join(WRK_DIR, lig_sdf_filename.replace('.sdf', '_prepared.pdbqt'))\n",
        "\n",
        "with open(lig_pdbqt_file_path, 'w') as pdbqt_file:\n",
        "    pdbqt_file.write(meeko_prep.write_pdbqt_string())\n",
        "\n",
        "print(f\"‚úÖ Ligand prepared and saved in working directory: `{lig_pdbqt_file_path}`\")\n",
        "\n",
        "# ------------------------\n",
        "#  Add py3Dmol visualization\n",
        "# ------------------------\n",
        "try:\n",
        "    import py3Dmol\n",
        "except ImportError:\n",
        "    print(\"Installing py3Dmol ...\")\n",
        "    !pip install py3Dmol\n",
        "    import py3Dmol\n",
        "\n",
        "# Read the generated SDF for visualization\n",
        "with open(LIG_sdf_lFile, 'r') as file:\n",
        "    sdf_data = file.read()\n",
        "\n",
        "viewer = py3Dmol.view(width=400, height=400)\n",
        "viewer.addModel(sdf_data, 'sdf')\n",
        "viewer.setStyle({'stick': {'colorScheme': 'lightGreyCarbon'}})\n",
        "viewer.zoomTo()\n",
        "viewer.show()\n"
      ],
      "metadata": {
        "id": "N7ACG8hiEEVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üß™ (Optional): If you have an initial ligand conformation, you may want to refine it.  ‚öõÔ∏è** { display-mode: \"form\" }\n",
        "# @markdown If you don't have it, skip this step.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Upload the Ligand File\n",
        "# -------------------------------\n",
        "print(\"\\nüì§ Please upload your ligand file (SDF format):\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ensure only one file is uploaded\n",
        "lig_sdf_filename = list(uploaded.keys())[0]\n",
        "uploaded_file_path = os.path.join(\"/content\", lig_sdf_filename)  # File initially goes to /content/\n",
        "\n",
        "\n",
        "\n",
        "# Move ligand file to the working directory (WRK_DIR)\n",
        "LIG_sdf_lFile = os.path.join(WRK_DIR, lig_sdf_filename)\n",
        "shutil.move(uploaded_file_path, LIG_sdf_lFile)\n",
        "print(f\"‚úÖ Ligand moved to working directory: `{LIG_sdf_lFile}`\")\n",
        "\n",
        "# -------------------------------\n",
        "# Refinement parameters\n",
        "# -------------------------------\n",
        "refine_forcefield = 'GAFF'  # @param ['GAFF', 'Ghemical', 'MMFF94', 'MMFF94s', 'UFF']\n",
        "refine_convergence = 0.00001\n",
        "refine_max_steps = 1000000\n",
        "refined_ligand_name = \"sr3420_refined\"\n",
        "\n",
        "# Define the output file path for the refined ligand\n",
        "refined_outfilename = os.path.join(WRK_DIR, f\"{refined_ligand_name}_docking_input.sdf\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Refine the ligand\n",
        "# -------------------------------\n",
        "if not os.path.exists(LIG_sdf_lFile):\n",
        "    print(\"‚ùå Error: Uploaded ligand file does not exist.\")\n",
        "else:\n",
        "    print(\"Refining with OpenBabel...\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load the molecule from the uploaded SDF file\n",
        "    mol = next(pybel.readfile(\"sdf\", LIG_sdf_lFile))\n",
        "\n",
        "    # Optimize the molecule using the selected force field and parameters\n",
        "    mol.localopt(forcefield=refine_forcefield, steps=refine_max_steps)\n",
        "\n",
        "    # Write the refined structure to the output file in SDF format\n",
        "    mol.write(\"sdf\", refined_outfilename, overwrite=True)\n",
        "    print(f\"OpenBabel: Refined structure saved to {refined_outfilename}\")\n",
        "\n",
        "    # Update LIG_sdf_lFile to point to the refined ligand file\n",
        "    LIG_sdf_lFile = refined_outfilename\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 3: Display the Refined Structure\n",
        "    # -------------------------------\n",
        "    try:\n",
        "        import py3Dmol\n",
        "    except ImportError:\n",
        "        print(\"Installing py3Dmol ...\")\n",
        "        !pip install py3Dmol\n",
        "        import py3Dmol\n",
        "\n",
        "    print(\"üîç Displaying the refined ligand structure...\")\n",
        "    with open(LIG_sdf_lFile, 'r') as file:\n",
        "        sdf_data = file.read()\n",
        "\n",
        "    viewer = py3Dmol.view(width=600, height=600)\n",
        "    viewer.addModel(sdf_data, 'sdf')\n",
        "    viewer.setStyle({'stick': {'colorScheme': 'lightGreyCarbon'}})\n",
        "    viewer.zoomTo()\n",
        "    viewer.show()"
      ],
      "metadata": {
        "id": "zSZS0jJ2DzKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚öóÔ∏è **Run this cell to install necessary packages to generate the ligand initial conformation** { display-mode: \"form\" }\n",
        "\n",
        "!pip install ipywidgets  > /dev/null 2>&1\n",
        "!jupyter nbextension enable --py widgetsnbextension > /dev/null 2>&1\n",
        "!jupyter nbextension install --py widgetsnbextension > /dev/null 2>&1\n",
        "!jupyter nbextension enable --py widgetsnbextension --sys-prefix > /dev/null 2>&1\n",
        "\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import ipywidgets as widgets\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"‚úÖ Packages installed successfully.\")"
      ],
      "metadata": {
        "id": "D-fQM-7TDzOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚öóÔ∏è **Ligand Structure Generator** { display-mode: \"form\" }\n",
        "#@markdown Paste IUPAC, or SMILES, or upload txt with IUPAC, and press <span style='display:inline-block; background-color:#28a745; padding:5px 10px; border-radius:4px; color:white;'>Generate Structure</span>.\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import requests\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "############################\n",
        "# Global SMILES_input variable\n",
        "############################\n",
        "SMILES_input = None\n",
        "\n",
        "# Function to convert IUPAC to SMILES using OPSIN API\n",
        "def iupac_to_smiles(iupac_name):\n",
        "    print(f\"üîç Converting IUPAC name to SMILES: {iupac_name}\")\n",
        "    opsin_url = f\"https://opsin.ch.cam.ac.uk/opsin/{iupac_name}.smi\"\n",
        "    response = requests.get(opsin_url)\n",
        "    if response.status_code == 200 and response.text.strip():\n",
        "        print(\"‚úÖ OPSIN API successfully returned a SMILES string.\")\n",
        "        return response.text.strip()\n",
        "    else:\n",
        "        print(\"‚ùå OPSIN API failed to return a valid SMILES string.\")\n",
        "        return None\n",
        "\n",
        "# Function to display molecule structure\n",
        "def draw_molecule(smiles):\n",
        "    print(f\"üß™ Drawing molecule for SMILES: {smiles}\")\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        img = Draw.MolToImage(mol, size=(400, 400))\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        print(\"‚úÖ Molecule structure displayed successfully.\")\n",
        "    else:\n",
        "        print(\"‚ùå Invalid SMILES provided.\")\n",
        "\n",
        "# Instruction Text\n",
        "instruction_text = widgets.HTML(\n",
        "    value=\"\"\"\n",
        "    <b>üí° How to Provide Your Ligand:</b><br>\n",
        "    - Enter the <b>IUPAC</b> name directly.<br>\n",
        "    - Upload a <b>.txt</b> file with the IUPAC name.<br>\n",
        "    - Or enter the <b>SMILES</b> string if available.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Input method selection\n",
        "input_choice = widgets.ToggleButtons(\n",
        "    options=['Enter IUPAC', 'Upload IUPAC File', 'Enter SMILES'],\n",
        "    description='Input Method:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Text input for IUPAC/SMILES\n",
        "text_input = widgets.Text(\n",
        "    placeholder='Enter IUPAC name or SMILES',\n",
        "    description='Input:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "# File upload widget\n",
        "file_upload = widgets.FileUpload(\n",
        "    accept='.txt',\n",
        "    multiple=False\n",
        ")\n",
        "\n",
        "# Output widget for dynamic display\n",
        "output = widgets.Output()\n",
        "\n",
        "# Main processing function\n",
        "def process_input(b):\n",
        "    global SMILES_input  # ensure we can assign to the global variable\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        print(\"‚öôÔ∏è Starting input processing...\")\n",
        "        print(f\"üîÑ Processing input using method: {input_choice.value}\")\n",
        "\n",
        "        if input_choice.value == 'Enter IUPAC':\n",
        "            if not text_input.value.strip():\n",
        "                print(\"‚ö†Ô∏è Please enter a valid IUPAC name.\")\n",
        "                return\n",
        "            smiles = iupac_to_smiles(text_input.value.strip())\n",
        "            if smiles:\n",
        "                SMILES_input = smiles  # Store in the global variable\n",
        "                print(f\"‚úÖ **Generated SMILES:** `{smiles}`\")\n",
        "                print(f\"üìå SMILES_input variable updated: {SMILES_input}\")\n",
        "                draw_molecule(smiles)\n",
        "            else:\n",
        "                print(\"‚ùå Could not retrieve SMILES from OPSIN. Please check the IUPAC name.\")\n",
        "\n",
        "        elif input_choice.value == 'Upload IUPAC File':\n",
        "            if file_upload.value:\n",
        "                content = next(iter(file_upload.value.values()))['content'].decode().strip()\n",
        "                print(f\"üìÑ File content: {content}\")\n",
        "                smiles = iupac_to_smiles(content)\n",
        "                if smiles:\n",
        "                    SMILES_input = smiles\n",
        "                    print(f\"‚úÖ **Generated SMILES from File:** `{smiles}`\")\n",
        "                    print(f\"üìå SMILES_input variable updated: {SMILES_input}\")\n",
        "                    draw_molecule(smiles)\n",
        "                else:\n",
        "                    print(\"‚ùå Could not retrieve SMILES from the uploaded file. Please verify the IUPAC name.\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Please upload a valid IUPAC `.txt` file.\")\n",
        "\n",
        "        elif input_choice.value == 'Enter SMILES':\n",
        "            if not text_input.value.strip():\n",
        "                print(\"‚ö†Ô∏è Please enter a valid SMILES string.\")\n",
        "                return\n",
        "            smiles = text_input.value.strip()\n",
        "            SMILES_input = smiles\n",
        "            print(f\"‚úÖ Received SMILES input: {smiles}\")\n",
        "            print(f\"üìå SMILES_input variable updated: {SMILES_input}\")\n",
        "            draw_molecule(smiles)\n",
        "\n",
        "        print(\"‚úÖ Processing completed.\")\n",
        "\n",
        "# Submit button\n",
        "submit_button = widgets.Button(\n",
        "    description='Generate Structure',\n",
        "    button_style='success',\n",
        "    tooltip='Click to generate the ligand structure'\n",
        ")\n",
        "submit_button.on_click(process_input)\n",
        "\n",
        "# Displaying the UI\n",
        "ui = widgets.VBox([\n",
        "    instruction_text,\n",
        "    input_choice,\n",
        "    text_input,\n",
        "    file_upload,\n",
        "    submit_button,\n",
        "    output\n",
        "])\n",
        "\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "WMeYjEXODzQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚öóÔ∏è **Generate Ligand Conformation** { display-mode: \"form\" }\n",
        "#@markdown Fill in the fields below and press **Run** to generate your 3D ligand conformation using Open Babel.\n",
        "\n",
        "Force_field = 'GAFF' # @param ['GAFF', 'Ghemical', 'MMFF94', 'MMFF94s', 'UFF']\n",
        "Convergence_criteria = '0.00001' # @param ['0.1', '0.01','0.001', '0.0001', '0.00001', '0.000001', '0.0000001']\n",
        "Maximum_steps = 1000000 # @param\n",
        "ligand_name = 'sr3420' #@param {type:\"string\"}\n",
        "\n",
        "def check_convergence(log_file: str) -> None:\n",
        "    with open(log_file, 'r') as inpt:\n",
        "        data = inpt.read()\n",
        "    if 'CONVERGED' in data:\n",
        "        verb = 'has'\n",
        "        step = data.split('\\n')[-4].split()[0]\n",
        "        step_str = f'({step} steps)'\n",
        "    else:\n",
        "        verb = 'has not'\n",
        "        step_str = ''\n",
        "    print(f'+ Steepest gradient {verb} converged {step_str}')\n",
        "\n",
        "print(f'+ Selected {Force_field} for energy minimisation '\n",
        "      f'up to {Convergence_criteria} iteration difference or '\n",
        "      f'at most {Maximum_steps:,} steps')\n",
        "\n",
        "LIG_sdf = ligand_name + \".sdf\"\n",
        "LIG_sdf_lFile = os.path.join(WRK_DIR, LIG_sdf)\n",
        "LIG_obmin_log = ligand_name + \"_obmin.log\"\n",
        "LIG_obmin_log_lFile = os.path.join(WRK_DIR, LIG_obmin_log)\n",
        "\n",
        "!obabel -:{\"\\\"\"+SMILES_input+\"\\\"\"} -O {LIG_sdf_lFile} --title {ligand_name} --gen3d \\\n",
        "--best --minimize --ff {Force_field} --steps {Maximum_steps} --sd \\\n",
        "--crit {Convergence_criteria} --log &> {LIG_obmin_log_lFile}\n",
        "\n",
        "check_convergence(LIG_obmin_log_lFile)\n",
        "print(f'+ {LIG_sdf} > saved in {WRK_DIR}')\n",
        "\n",
        "# ------------------------\n",
        "#  Add py3Dmol visualization\n",
        "# ------------------------\n",
        "try:\n",
        "    import py3Dmol\n",
        "except ImportError:\n",
        "    print(\"Installing py3Dmol ...\")\n",
        "    !pip install py3Dmol\n",
        "    import py3Dmol\n",
        "\n",
        "# Read the generated SDF for visualization\n",
        "with open(LIG_sdf_lFile, 'r') as file:\n",
        "    sdf_data = file.read()\n",
        "\n",
        "viewer = py3Dmol.view(width=600, height=600)\n",
        "viewer.addModel(sdf_data, 'sdf')\n",
        "viewer.setStyle({'stick': {'colorScheme': 'lightGreyCarbon'}})\n",
        "viewer.zoomTo()\n",
        "viewer.show()"
      ],
      "metadata": {
        "id": "ZcZTex7uDzSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ü©∫ **Ligand Conformation Validation with PoseBusters** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ‚úÖ **Instructions:**\n",
        "#@markdown - Click the button below to **validate the chemical conformation** of the generated ligand using **PoseBusters**.\n",
        "#@markdown - If **all tests pass**, you're good to proceed to docking.\n",
        "#@markdown - If **any test fails**, please rerun the previous cell, hopefully next run will be luckier üòä\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown üëâ **Note:** Ensuring valid ligand conformations is crucial for reliable docking results.\n",
        "\n",
        "\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    LIG_sdf_lFile\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è The variable 'LIG_sdf_lFile' is not defined. Please run the ligand generation cell first.\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    import posebusters\n",
        "except ImportError:\n",
        "    print(\"Installing PoseBusters ...\")\n",
        "    !pip install posebusters > /dev/null 2>&1\n",
        "\n",
        "# Run PoseBusters on the generated SDF\n",
        "print(f\"\\nüî¨ Running PoseBusters on: {LIG_sdf_lFile}\")\n",
        "result = subprocess.run([\"bust\", LIG_sdf_lFile], capture_output=True, text=True)\n",
        "output = result.stdout + \"\\n\" + result.stderr\n",
        "\n",
        "print(\"===== PoseBusters Output =====\")\n",
        "print(output)\n",
        "print(\"================================\\n\")\n",
        "\n",
        "# 3) Check for 'passes (X / Y)'\n",
        "match = re.search(r'passes \\((\\d+) / (\\d+)\\)', output)\n",
        "if match:\n",
        "    passed_tests = int(match.group(1))\n",
        "    total_tests = int(match.group(2))\n",
        "\n",
        "    if passed_tests == total_tests:\n",
        "        print(f\"‚úÖ All {total_tests} PoseBusters tests passed!\")\n",
        "        print(\"Everything is fine. You may proceed to docking. üöÄ\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Not all tests passed: {passed_tests}/{total_tests}\")\n",
        "        print(\"Please re-run the previous cell to regenerate the ligand, then check again.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not parse PoseBusters output. Tests may have failed or an error occurred.\")\n",
        "    print(\"Please review the output above or re-run the previous cell.\")\n"
      ],
      "metadata": {
        "id": "ChLlIC7BEf_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß™ **Ligand Preparation for Docking with Meeko** { display-mode: \"form\" }\n",
        "#@markdown This step will:\n",
        "#@markdown - **Load** the ligand from the generated SDF file.\n",
        "#@markdown - **Embed** a new conformer if none are detected.\n",
        "#@markdown - **Prepare** the ligand using Meeko.\n",
        "#@markdown - **Save** the prepared ligand as a PDBQT file.\n",
        "\n",
        "#@markdown ‚úÖ The process status will be displayed below.\n",
        "\n",
        "import os\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, AddHs, MolFromMolFile\n",
        "from meeko import MoleculePreparation\n",
        "\n",
        "lig = MolFromMolFile(LIG_sdf_lFile, sanitize=True)\n",
        "assert lig is not None, f\"Failed to load ligand from {LIG_sdf_lFile}\"\n",
        "\n",
        "if lig.GetNumConformers() == 0:\n",
        "    print(\"No conformers detected, embedding a new conformer...\")\n",
        "    AllChem.EmbedMolecule(lig, randomSeed=0xF00D)\n",
        "\n",
        "lig = AddHs(lig, addCoords=True)\n",
        "\n",
        "meeko_prep = MoleculePreparation()\n",
        "meeko_prep.prepare(lig)\n",
        "\n",
        "lig_pdbqt_file_path = LIG_sdf_lFile.replace(\".sdf\", \"_ligand_start_conf_prepared.pdbqt\")\n",
        "\n",
        "with open(lig_pdbqt_file_path, \"w\") as pdbqt_file:\n",
        "    pdbqt_file.write(meeko_prep.write_pdbqt_string())\n",
        "\n",
        "print(f\"‚úÖ PDBQT generated at: {lig_pdbqt_file_path}\")\n"
      ],
      "metadata": {
        "id": "dBpjJtn_EgCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **üöÄ Perform RAPID-Net Guided Docking with AutoDock Vina üéØ** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown üìÇ **Output Formats:**\n",
        "#@markdown - For the **PoseBusters dataset**, resulting poses are saved in **`.sdf`** format.\n",
        "#@markdown - For **completeness**, the poses are also saved in **`.pdb`** format.\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from meeko import PDBQTMolecule, RDKitMolCreate\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import SDWriter\n",
        "\n",
        "from meeko import MoleculePreparation, PDBQTMolecule, RDKitMolCreate\n",
        "from rdkit.Chem import AddHs, MolFromMolFile, SDWriter\n",
        "import subprocess\n",
        "\n",
        "!rm -r /content/sample_data\n",
        "!wget https://github.com/ccsb-scripps/AutoDock-Vina/releases/download/v1.2.7/vina_1.2.7_linux_x86_64 -O vina\n",
        "!chmod u+x vina\n",
        "\n",
        "%alias vina /content/vina\n",
        "\n",
        "cpu_cores = os.cpu_count()\n",
        "\n",
        "Exhaustiveness = 32  # Ensure this is an integer\n",
        "num_modes = 40\n",
        "\n",
        "# **Restrict execution to only WRK_DIR**\n",
        "subfolder_path = WRK_DIR  # Use WRK_DIR instead of looping through multiple subfolders\n",
        "\n",
        "def extract_vina_scores(log_file, output_csv):\n",
        "    with open(log_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    scores = []\n",
        "    start_collecting = False\n",
        "    for line in lines:\n",
        "        if start_collecting:\n",
        "            parts = line.split()\n",
        "            if len(parts) == 4 and parts[0].isdigit():\n",
        "                scores.append({\n",
        "                    \"NAME\": f\"{os.path.basename(log_file).split('_')[0]}_{parts[0]}\",\n",
        "                    \"DOCK_SC\": parts[1],\n",
        "                    \"RMSD_LB\": parts[2],\n",
        "                    \"RMSD_UB\": parts[3]\n",
        "                })\n",
        "\n",
        "        if \"mode |   affinity | dist from best mode\" in line:\n",
        "            start_collecting = True\n",
        "\n",
        "    df = pd.DataFrame(scores)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f'+ {output_csv} > DOCKING folder')\n",
        "\n",
        "# Ensure WRK_DIR exists before processing\n",
        "if os.path.isdir(subfolder_path):\n",
        "    for pocket_file in os.listdir(subfolder_path):\n",
        "        if (\"Minimal\" in pocket_file or \"Majority\" in pocket_file) and pocket_file.endswith(\".pdb\"):\n",
        "            pocket_name = os.path.splitext(pocket_file)[0]\n",
        "\n",
        "            # Select the correct threshold list based on pocket type\n",
        "            if \"Minimal\" in pocket_file:\n",
        "                thresholds = minimal_thresholds\n",
        "            elif \"Majority\" in pocket_file:\n",
        "                thresholds = majority_thresholds\n",
        "            else:\n",
        "                continue  # Skip unknown pockets\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                protein_file = prot_pdbqt_file_path\n",
        "                ligand_file = lig_pdbqt_file_path\n",
        "\n",
        "                config_file = os.path.join(subfolder_path, f\"{pocket_name}_config_{threshold}.txt\")\n",
        "                docking_folder = os.path.join(subfolder_path, f\"{pocket_name}_results_{threshold}\")\n",
        "\n",
        "                if not os.path.exists(protein_file) or not os.path.exists(ligand_file):\n",
        "                    print(f\"‚ùå Protein or ligand file missing in {WRK_DIR}. Skipping docking.\")\n",
        "                    continue\n",
        "\n",
        "                ID = f\"{os.path.basename(subfolder_path)}_{pocket_name}_{threshold}\"\n",
        "                oupt_log = f\"{ID}_output.log\"\n",
        "                oupt_pdbqt = f\"{ID}_output.pdbqt\"\n",
        "                oupt_log_dFFile = os.path.join(docking_folder, oupt_log)\n",
        "                oupt_pdbqt_dFFile = os.path.join(docking_folder, oupt_pdbqt)\n",
        "\n",
        "                print(f\"üöÄ Starting docking for {ID}...\")\n",
        "\n",
        "                # -- Start docking --\n",
        "                start_time = time.time()\n",
        "                %vina --receptor {protein_file} --ligand {ligand_file} \\\n",
        "                --out {oupt_pdbqt_dFFile} --config {config_file} --cpu {cpu_cores} \\\n",
        "                --exhaustiveness {Exhaustiveness} --num_modes {num_modes} --verbosity 2 | tee {oupt_log_dFFile}\n",
        "                end_time = time.time()  # Fix: Ensure end_time is set right after docking\n",
        "                # -- End docking --\n",
        "\n",
        "                print(f\"‚úÖ Docking completed for {ID} in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "                with open(oupt_pdbqt_dFFile, 'r') as oupt:\n",
        "                    output_pdbqt = oupt.read()\n",
        "\n",
        "                pdbqt_mol = PDBQTMolecule(output_pdbqt)\n",
        "                rdkit_mol = RDKitMolCreate.from_pdbqt_mol(pdbqt_mol)[0]\n",
        "\n",
        "                for conf_id in range(rdkit_mol.GetNumConformers()):\n",
        "                    LIG_dash_sdf = f\"{ID}_pose_{conf_id + 1}.sdf\"\n",
        "                    LIG_dash_sdf_dFFile = os.path.join(docking_folder, LIG_dash_sdf)\n",
        "                    writer = SDWriter(str(LIG_dash_sdf_dFFile))\n",
        "                    writer.write(rdkit_mol, confId=conf_id)\n",
        "                    writer.close()\n",
        "\n",
        "                # Save as PDB as well.\n",
        "                for conf_id in range(rdkit_mol.GetNumConformers()):\n",
        "                    LIG_dash_pdb = f\"{ID}_pose_{conf_id + 1}.pdb\"\n",
        "                    LIG_dash_pdb_dFFile = os.path.join(docking_folder, LIG_dash_pdb)\n",
        "                    Chem.MolToPDBFile(rdkit_mol, LIG_dash_pdb_dFFile, confId=conf_id)\n",
        "\n",
        "\n",
        "                extract_vina_scores(oupt_log_dFFile, os.path.join(docking_folder, f\"{ID}_dockrpt.csv\"))\n",
        "\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Working directory {WRK_DIR} does not exist. Skipping.\")\n"
      ],
      "metadata": {
        "id": "OH0WBzAEEgGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üî¨ Select the Most Energetically Favorable Pose Based on Vina Scoring üìä** { display-mode: \"form\" }\n",
        "#@markdown üíæ **Output Formats:** both **`.sdf`** and **`.pdb`**\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "base_dir = WRK_DIR\n",
        "\n",
        "docking_scores = {}\n",
        "\n",
        "def list_files_in_directory(directory, extension):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(extension):\n",
        "                file_paths.append(os.path.join(root, file))\n",
        "    return file_paths\n",
        "\n",
        "csv_files = list_files_in_directory(base_dir, \"_dockrpt.csv\")\n",
        "\n",
        "print(f\"All CSV files found in {base_dir} and subdirectories:\")\n",
        "for file in csv_files:\n",
        "    print(file)\n",
        "\n",
        "for file_path in csv_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"CSV Columns in {file_path}: {df.columns}\")\n",
        "        if 'DOCK_SC' in df.columns:\n",
        "            scores = df['DOCK_SC'].astype(float).tolist()\n",
        "            for score in scores:\n",
        "                docking_scores[score] = file_path\n",
        "            print(f\"Found docking scores in {file_path}: {scores}\")\n",
        "        else:\n",
        "            print(f\"DOCK_SC column not found in {file_path}\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"CSV file is empty: {file_path}\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(f\"CSV file is malformed: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file {file_path}: {e}\")\n",
        "\n",
        "print(f\"Total number of docking scores found: {len(docking_scores)}\")\n",
        "print(\"Docking scores:\", list(docking_scores.keys()))\n",
        "\n",
        "if docking_scores:\n",
        "    # Select the most energetically favorable (lowest) docking score\n",
        "    most_favorable_score = min(docking_scores.keys())\n",
        "    most_favorable_file = docking_scores[most_favorable_score]\n",
        "    print(f\"Most energetically favorable docking score: {most_favorable_score}\")\n",
        "    print(f\"File containing the most energetically favorable pose: {most_favorable_file}\")\n",
        "\n",
        "    # Extract the folder path containing the best docking report file\n",
        "    most_favorable_folder = os.path.dirname(most_favorable_file)\n",
        "    print(f\"Folder containing the most energetically favorable pose: {most_favorable_folder}\")\n",
        "\n",
        "    # Initialize variables to store both SDF and PDB pose file paths\n",
        "    leading_pose_sdf_file = None\n",
        "    leading_pose_pdb_file = None\n",
        "\n",
        "    # Walk the directory to find both pose files\n",
        "    for root, dirs, files in os.walk(most_favorable_folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\"_pose_1.sdf\"):\n",
        "                leading_pose_sdf_file = os.path.join(root, file)\n",
        "            elif file.endswith(\"_pose_1.pdb\"):\n",
        "                leading_pose_pdb_file = os.path.join(root, file)\n",
        "        if leading_pose_sdf_file and leading_pose_pdb_file:\n",
        "            break\n",
        "\n",
        "    if leading_pose_sdf_file and leading_pose_pdb_file:\n",
        "        print(f\"Leading binding pose SDF file found: {leading_pose_sdf_file}\")\n",
        "        print(f\"Leading binding pose PDB file found: {leading_pose_pdb_file}\")\n",
        "        protein_name = os.path.basename(base_dir)\n",
        "\n",
        "        # Copy the SDF file\n",
        "        new_sdf_file_name = f\"{protein_name}_predicted_pose.sdf\"\n",
        "        predicted_pose_path_sdf = os.path.join(base_dir, new_sdf_file_name)\n",
        "        shutil.copy(leading_pose_sdf_file, predicted_pose_path_sdf)\n",
        "        print(f\"Leading binding pose SDF file copied to: {predicted_pose_path_sdf}\")\n",
        "\n",
        "        # Copy the PDB file\n",
        "        new_pdb_file_name = f\"{protein_name}_predicted_pose.pdb\"\n",
        "        predicted_pose_path_pdb = os.path.join(base_dir, new_pdb_file_name)\n",
        "        shutil.copy(leading_pose_pdb_file, predicted_pose_path_pdb)\n",
        "        print(f\"Leading binding pose PDB file copied to: {predicted_pose_path_pdb}\")\n",
        "    else:\n",
        "        if not leading_pose_sdf_file:\n",
        "            print(\"No leading binding pose SDF file found ending with '_pose_1.sdf'.\")\n",
        "        if not leading_pose_pdb_file:\n",
        "            print(\"No leading binding pose PDB file found ending with '_pose_1.pdb'.\")\n",
        "else:\n",
        "    print(\"No docking scores found.\")\n",
        "\n",
        "\n",
        "# === Additionally: Generate Poses with Added Hydrogens ===\n",
        "\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "except ImportError:\n",
        "    raise ImportError(\"RDKit is required for hydrogen addition. Please install it before running this code.\")\n",
        "\n",
        "try:\n",
        "    sdf_supplier = Chem.SDMolSupplier(predicted_pose_path_sdf, removeHs=False)\n",
        "    mol = sdf_supplier[0] if sdf_supplier and len(sdf_supplier) > 0 else None\n",
        "    if mol is not None:\n",
        "        mol_with_H = Chem.AddHs(mol)\n",
        "        predicted_pose_path_sdf_H = os.path.join(base_dir, f\"{protein_name}_predicted_pose_H.sdf\")\n",
        "        writer = Chem.SDWriter(predicted_pose_path_sdf_H)\n",
        "        writer.write(mol_with_H)\n",
        "        writer.close()\n",
        "        print(f\"Hydrogen-added SDF pose file generated: {predicted_pose_path_sdf_H}\")\n",
        "    else:\n",
        "        print(\"Failed to load molecule from SDF for hydrogen addition.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing SDF for hydrogen addition: {e}\")\n",
        "\n",
        "\n",
        "try:\n",
        "    mol = Chem.MolFromPDBFile(predicted_pose_path_pdb, removeHs=False)\n",
        "    if mol is not None:\n",
        "        mol_with_H = Chem.AddHs(mol)\n",
        "        predicted_pose_path_pdb_H = os.path.join(base_dir, f\"{protein_name}_predicted_pose_H.pdb\")\n",
        "        Chem.MolToPDBFile(mol_with_H, predicted_pose_path_pdb_H)\n",
        "        print(f\"Hydrogen-added PDB pose file generated: {predicted_pose_path_pdb_H}\")\n",
        "    else:\n",
        "        print(\"Failed to load molecule from PDB for hydrogen addition.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing PDB for hydrogen addition: {e}\")\n"
      ],
      "metadata": {
        "id": "S_osuq81EgJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ü©∫ **Check Top-1 Vina Pose with PoseBusters** { display-mode: \"form\" }\n",
        "#@markdown Press \"Run\" to validate the selected **leading binding pose** with PoseBusters.\n",
        "\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    predicted_pose_path_sdf\n",
        "    prot_pdb_rm_bad_path\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è 'predicted_pose_path_sdf' or 'prot_pdb_rm_bad_path' not defined. Run the previous cell first.\")\n",
        "    raise\n",
        "\n",
        "print(f\"üîé Checking pose at:\\n  {predicted_pose_path_sdf}\")\n",
        "print(f\"üîé Using protein file at:\\n  {prot_pdb_rm_bad_path}\\n\")\n",
        "\n",
        "try:\n",
        "    import posebusters\n",
        "except ImportError:\n",
        "    print(\"Installing PoseBusters ...\")\n",
        "    !pip install posebusters\n",
        "\n",
        "cmd = [\"bust\", predicted_pose_path_sdf, \"-p\", prot_pdb_rm_bad_path]\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "pb_output = result.stdout + \"\\n\" + result.stderr\n",
        "\n",
        "print(\"===== PoseBusters Output =====\")\n",
        "print(pb_output)\n",
        "print(\"==============================\\n\")\n",
        "\n",
        "# Look for 'passes (X / X)'\n",
        "match = re.search(r'passes \\((\\d+) / (\\d+)\\)', pb_output)\n",
        "if match:\n",
        "    passed_tests = int(match.group(1))\n",
        "    total_tests = int(match.group(2))\n",
        "    if passed_tests == total_tests:\n",
        "        print(f\"‚úÖ All {total_tests} PoseBusters tests passed!\")\n",
        "        print(\"Everything is fine. You may proceed to the next step.\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Not all tests passed: {passed_tests}/{total_tests}\")\n",
        "        print(\"Please consider re-checking your ligand or re-running docking.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not parse PoseBusters test results.\")\n",
        "    print(\"Please review the output above or re-run the analysis.\")\n"
      ],
      "metadata": {
        "id": "75dHojt1EgN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üñºÔ∏èüé®üîç Your protein (ab5_ckfix.pdb in this case) with Majority-voted pockets, their labels & predicted binding pose** { display-mode: \"form\" }\n",
        "\n",
        "import py3Dmol\n",
        "import numpy as np\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "from rdkit import Chem  # Required to load the predicted pose\n",
        "\n",
        "# Option to show pocket labels (True to show, False to hide)\n",
        "show_pocket_labels = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# Find all pocket files in WRK_DIR that match \"pocket...Majority...pdb\"\n",
        "cyan_pocket_paths = [\n",
        "    os.path.join(WRK_DIR, file) for file in os.listdir(WRK_DIR)\n",
        "    if file.startswith(\"pocket\") and \"Majority\" in file and file.endswith(\".pdb\")\n",
        "]\n",
        "\n",
        "# Function to compute the geometric center of an entire pocket\n",
        "def compute_pocket_center(pocket_path):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"pocket\", pocket_path)\n",
        "    coords = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                for atom in residue:\n",
        "                    if atom.element != 'H':\n",
        "                        coords.append(atom.coord)\n",
        "    if coords:\n",
        "        return np.mean(coords, axis=0)\n",
        "    return None\n",
        "\n",
        "# Parse the main protein\n",
        "parser = PDBParser(QUIET=True)\n",
        "structure = parser.get_structure(\"protein\", protein_file)\n",
        "\n",
        "viewer = py3Dmol.view(width=1500, height=900)\n",
        "\n",
        "with open(protein_file, 'r') as protein_data:\n",
        "    viewer.addModel(protein_data.read(), 'pdb')\n",
        "viewer.setStyle({'model': 0}, {\"cartoon\": {\"color\": \"spectrum\"}})\n",
        "\n",
        "model_index = 1\n",
        "\n",
        "for pocket_path in cyan_pocket_paths:\n",
        "    with open(pocket_path, 'r') as pocket_data:\n",
        "        viewer.addModel(pocket_data.read(), 'pdb')\n",
        "\n",
        "    viewer.setStyle(\n",
        "        {'model': model_index},\n",
        "        {'stick': {'color': 'cyan', 'radius': 0.1, 'opacity': 0.8}}\n",
        "    )\n",
        "\n",
        "    # Compute & label pocket center if the user opted to show labels\n",
        "    if show_pocket_labels:\n",
        "        pocket_center = compute_pocket_center(pocket_path)\n",
        "        if pocket_center is not None:\n",
        "            x, y, z = map(float, pocket_center)\n",
        "            pocket_name = os.path.splitext(os.path.basename(pocket_path))[0]  # e.g. \"pocket_thr050_Minimal0\"\n",
        "            viewer.addLabel(\n",
        "                pocket_name,\n",
        "                {\n",
        "                    \"position\": {\"x\": x, \"y\": y, \"z\": z},\n",
        "                    \"fontColor\": \"black\",\n",
        "                    \"font\": \"Helvetica\",\n",
        "                    \"fontSize\": 14,\n",
        "                    \"backgroundColor\": \"white\",\n",
        "                    \"showBackground\": True,\n",
        "                    \"opacity\": 1.0,\n",
        "                    \"inFront\": True,\n",
        "                    \"screenOffset\": {\"x\": 10, \"y\": -10}\n",
        "                }\n",
        "            )\n",
        "\n",
        "    model_index += 1\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Add the predicted binding pose (green stick and sphere) with reduced opacity\n",
        "# -------------------------------------------------\n",
        "predicted_pose = Chem.MolFromMolFile(predicted_pose_path_sdf)\n",
        "if predicted_pose is not None:\n",
        "    predicted_pose_block = Chem.MolToMolBlock(predicted_pose)\n",
        "    viewer.addModel(predicted_pose_block, 'mol')\n",
        "    viewer.setStyle(\n",
        "        {'model': model_index},\n",
        "        {\n",
        "            \"stick\": {\"colorscheme\": \"greenCarbon\", \"opacity\": 0.6},\n",
        "            \"sphere\": {\"scale\": 0.3, \"opacity\": 1.0}\n",
        "        }\n",
        "    )\n",
        "    model_index += 1\n",
        "\n",
        "viewer.zoomTo()\n",
        "viewer.show()"
      ],
      "metadata": {
        "id": "uxuqMH4sEsCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üñºÔ∏èüé®üîç Your protein (ab5_ckfix.pdb in this case) with Minimally-reported pockets, their labels & predicted binding pose** { display-mode: \"form\" }\n",
        "\n",
        "import py3Dmol\n",
        "import numpy as np\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "from rdkit import Chem  # Required to load the predicted pose\n",
        "\n",
        "# Option to show pocket labels (True to show, False to hide)\n",
        "show_pocket_labels = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# Find all pocket files in WRK_DIR that match \"pocket...Majority...pdb\"\n",
        "cyan_pocket_paths = [\n",
        "    os.path.join(WRK_DIR, file) for file in os.listdir(WRK_DIR)\n",
        "    if file.startswith(\"pocket\") and \"Minimal\" in file and file.endswith(\".pdb\")\n",
        "]\n",
        "\n",
        "# Function to compute the geometric center of an entire pocket\n",
        "def compute_pocket_center(pocket_path):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"pocket\", pocket_path)\n",
        "    coords = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                for atom in residue:\n",
        "                    if atom.element != 'H':\n",
        "                        coords.append(atom.coord)\n",
        "    if coords:\n",
        "        return np.mean(coords, axis=0)\n",
        "    return None\n",
        "\n",
        "# Parse the main protein\n",
        "parser = PDBParser(QUIET=True)\n",
        "structure = parser.get_structure(\"protein\", protein_file)\n",
        "\n",
        "viewer = py3Dmol.view(width=1500, height=900)\n",
        "\n",
        "with open(protein_file, 'r') as protein_data:\n",
        "    viewer.addModel(protein_data.read(), 'pdb')\n",
        "viewer.setStyle({'model': 0}, {\"cartoon\": {\"color\": \"spectrum\"}})\n",
        "\n",
        "model_index = 1\n",
        "\n",
        "for pocket_path in cyan_pocket_paths:\n",
        "    with open(pocket_path, 'r') as pocket_data:\n",
        "        viewer.addModel(pocket_data.read(), 'pdb')\n",
        "\n",
        "    viewer.setStyle(\n",
        "        {'model': model_index},\n",
        "        {'stick': {'color': 'purple', 'radius': 0.3, 'opacity': 0.8}}\n",
        "    )\n",
        "\n",
        "    # Compute & label pocket center if the user opted to show labels\n",
        "    if show_pocket_labels:\n",
        "        pocket_center = compute_pocket_center(pocket_path)\n",
        "        if pocket_center is not None:\n",
        "            x, y, z = map(float, pocket_center)\n",
        "            pocket_name = os.path.splitext(os.path.basename(pocket_path))[0]  # e.g. \"pocket_thr050_Minimal0\"\n",
        "            viewer.addLabel(\n",
        "                pocket_name,\n",
        "                {\n",
        "                    \"position\": {\"x\": x, \"y\": y, \"z\": z},\n",
        "                    \"fontColor\": \"black\",\n",
        "                    \"font\": \"Helvetica\",\n",
        "                    \"fontSize\": 14,\n",
        "                    \"backgroundColor\": \"white\",\n",
        "                    \"showBackground\": True,\n",
        "                    \"opacity\": 1.0,\n",
        "                    \"inFront\": True,\n",
        "                    \"screenOffset\": {\"x\": 10, \"y\": -10}\n",
        "                }\n",
        "            )\n",
        "\n",
        "    model_index += 1\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Add the predicted binding pose (green stick and sphere) with reduced opacity\n",
        "# -------------------------------------------------\n",
        "predicted_pose = Chem.MolFromMolFile(predicted_pose_path_sdf)\n",
        "if predicted_pose is not None:\n",
        "    predicted_pose_block = Chem.MolToMolBlock(predicted_pose)\n",
        "    viewer.addModel(predicted_pose_block, 'mol')\n",
        "    viewer.setStyle(\n",
        "        {'model': model_index},\n",
        "        {\n",
        "            \"stick\": {\"colorscheme\": \"greenCarbon\", \"opacity\": 0.6},\n",
        "            \"sphere\": {\"scale\": 0.3, \"opacity\": 1.0}\n",
        "        }\n",
        "    )\n",
        "    model_index += 1\n",
        "\n",
        "viewer.zoomTo()\n",
        "viewer.show()\n"
      ],
      "metadata": {
        "id": "5bNbJJI6EsFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **üî¨‚öõÔ∏è Arrange Protein-Ligand Complex, üõ†Ô∏è Install PLIP, & üìä Profile Interactions** { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import importlib\n",
        "\n",
        "print(\"Installing PLIP...\")\n",
        "!pip install --no-cache-dir --upgrade plip > /dev/null 2>&1\n",
        "\n",
        "import plip\n",
        "import plip.basic.supplemental as sup\n",
        "from plip.exchange.report import BindingSiteReport\n",
        "from plip.structure.preparation import PDBComplex\n",
        "\n",
        "# --- Step 2: Fix OverflowError in PLIP's `int32_to_negative` ---\n",
        "def safe_int32_to_negative(val):\n",
        "    \"\"\"Safely convert uint32 to int32 to prevent OverflowError.\"\"\"\n",
        "    return int(np.int32(val))\n",
        "\n",
        "sup.int32_to_negative = safe_int32_to_negative\n",
        "importlib.reload(plip.basic.supplemental)\n",
        "\n",
        "\n",
        "def generate_cmpx_pdb(inpt_prot: str, inpt_pose: str, oupt_cmpx: str) -> None:\n",
        "    \"\"\"\n",
        "    Merges a protein PDB (ATOM lines) with a ligand PDB (HETATM lines)\n",
        "    into a single complex PDB file.\n",
        "    \"\"\"\n",
        "    def write_line(line: str, keywords: list, oupt_file) -> None:\n",
        "        if line[:6].strip() in keywords:\n",
        "            oupt_file.write(line)\n",
        "    with open(oupt_cmpx, 'w') as outF, \\\n",
        "         open(inpt_prot, 'r') as protF, \\\n",
        "         open(inpt_pose, 'r') as poseF:\n",
        "        for p_line in protF:\n",
        "            write_line(p_line, ['ATOM', 'CONECT', 'TER'], outF)\n",
        "        for l_line in poseF:\n",
        "            write_line(l_line, ['HETATM', 'CONECT', 'END'], outF)\n",
        "\n",
        "def remove_np_float64_wrappers(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Repeatedly remove all occurrences of np.float64(...) in a string.\n",
        "    \"\"\"\n",
        "    pattern = r\"np\\.float64\\(\\s*([^)]+)\\s*\\)\"\n",
        "    prev = None\n",
        "    cleaned = s\n",
        "    while cleaned != prev:\n",
        "        prev = cleaned\n",
        "        cleaned = re.sub(pattern, r\"\\1\", cleaned)\n",
        "    return cleaned\n",
        "\n",
        "def format_tuple_of_numbers(numbers):\n",
        "    \"\"\"\n",
        "    Given a tuple or list of numbers, returns a string:\n",
        "      - If all numbers are integral, e.g. (3586, 3587, 3588)\n",
        "      - Otherwise, rounds each to 3 decimals: e.g. (1.200, 4.287, -8.730)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        numbers = [float(x) for x in numbers]\n",
        "    except Exception:\n",
        "        return str(numbers)\n",
        "    if all(x.is_integer() for x in numbers):\n",
        "        ints = [int(x) for x in numbers]\n",
        "        return \"(\" + \", \".join(map(str, ints)) + \")\"\n",
        "    else:\n",
        "        rounded = [f\"{round(x, 3):.3f}\" for x in numbers]\n",
        "        return \"(\" + \", \".join(rounded) + \")\"\n",
        "\n",
        "def clean_coordinate(value):\n",
        "    \"\"\"\n",
        "    Reformats a coordinate string by ensuring proper comma spacing.\n",
        "    \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", value)\n",
        "        if match:\n",
        "            return \"(\" + \", \".join(match) + \")\"\n",
        "    return value\n",
        "\n",
        "def cleanup_cell(value):\n",
        "    \"\"\"\n",
        "    Cleans up a cell's content by:\n",
        "      - If it's a float, rounds it (or converts to int if integral).\n",
        "      - If it's a tuple or list of numbers, returns a formatted string.\n",
        "      - If it's a string, removes np.float64 wrappers and then attempts to parse.\n",
        "      - Otherwise, returns the value as-is.\n",
        "    \"\"\"\n",
        "    # If it's a float:\n",
        "    if isinstance(value, float):\n",
        "        if value.is_integer():\n",
        "            return int(value)\n",
        "        else:\n",
        "            return round(value, 3)\n",
        "    # If it's an int, return as-is:\n",
        "    if isinstance(value, int):\n",
        "        return value\n",
        "    # If it's a tuple or list, force format it:\n",
        "    if isinstance(value, (list, tuple)):\n",
        "        return format_tuple_of_numbers(value)\n",
        "    # Now if it's a string:\n",
        "    if isinstance(value, str):\n",
        "        # Remove np.float64 wrappers\n",
        "        cleaned = remove_np_float64_wrappers(value)\n",
        "        try:\n",
        "            parsed = ast.literal_eval(cleaned)\n",
        "            if isinstance(parsed, (list, tuple)) and all(isinstance(x, (int, float)) for x in parsed):\n",
        "                return format_tuple_of_numbers(parsed)\n",
        "            elif isinstance(parsed, (int, float)):\n",
        "                if isinstance(parsed, float) and parsed.is_integer():\n",
        "                    return int(parsed)\n",
        "                else:\n",
        "                    return round(parsed, 3)\n",
        "            else:\n",
        "                return str(parsed)\n",
        "        except:\n",
        "            return cleaned\n",
        "    # Fallback: return string representation\n",
        "    return str(value)\n",
        "\n",
        "def interaction_profiler(inpt_cmpx: str, oupt_csv: str) -> None:\n",
        "    \"\"\"\n",
        "    Uses PLIP to analyze protein-ligand interactions from the merged PDB (with ligand HETID='UNL'),\n",
        "    then exports a cleaned CSV of interactions.\n",
        "    \"\"\"\n",
        "    int_bonds = [\n",
        "        'HYDROPHOBIC', 'HBOND', 'WATERBRIDGE', 'SALTBRIDGE',\n",
        "        'PISTACKING', 'PICATION', 'HALOGEN', 'METAL'\n",
        "    ]\n",
        "\n",
        "    def BSR(inpt_pdb: str) -> BindingSiteReport:\n",
        "        try:\n",
        "            print(\"DEBUG: Loading complex PDB from:\", inpt_pdb)\n",
        "            cmpx_mol = PDBComplex()\n",
        "            cmpx_mol.load_pdb(inpt_pdb)\n",
        "            print(\"DEBUG: Ligands found in complex:\")\n",
        "            for lig in cmpx_mol.ligands:\n",
        "                print(f\"  - Ligand hetid='{lig.hetid}', chain='{getattr(lig, 'chain', 'N/A')}', residue='{getattr(lig, 'resnum', 'N/A')}'\")\n",
        "            matching_ligands = [lig for lig in cmpx_mol.ligands if lig.hetid == 'UNL']\n",
        "            if not matching_ligands:\n",
        "                raise ValueError(\"No ligand with hetid='UNL' found. Check your PDB.\")\n",
        "            cmpx_lig = matching_ligands[0]\n",
        "            ligand_name = getattr(cmpx_lig, \"name\", None)\n",
        "            print(\"DEBUG: Ligand Attributes:\")\n",
        "            print(f\"  - HETID: {cmpx_lig.hetid}\")\n",
        "            print(f\"  - Chain: {getattr(cmpx_lig, 'chain', 'N/A')}\")\n",
        "            print(f\"  - Residue Number: {getattr(cmpx_lig, 'resnum', 'N/A')}\")\n",
        "            print(f\"  - Name: {ligand_name if ligand_name is not None else 'N/A'} (If missing, will assign a fallback)\")\n",
        "            if ligand_name is None:\n",
        "                ligand_name = f\"{cmpx_lig.hetid}_{getattr(cmpx_lig, 'chain', 'N/A')}_{getattr(cmpx_lig, 'resnum', '1')}\"\n",
        "            print(f\"Characterizing ligand with ID: {ligand_name}...\")\n",
        "            cmpx_mol.characterize_complex(cmpx_lig)\n",
        "            available_keys = list(cmpx_mol.interaction_sets.keys())\n",
        "            print(\"DEBUG: Available interaction set keys:\", available_keys)\n",
        "            if ligand_name in cmpx_mol.interaction_sets:\n",
        "                key = ligand_name\n",
        "            elif len(available_keys) == 1:\n",
        "                key = available_keys[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Unable to find key '{ligand_name}' in interaction sets. Available keys: {available_keys}\")\n",
        "            return BindingSiteReport(cmpx_mol.interaction_sets[key])\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Issue with PLIP processing: {e}\")\n",
        "            raise\n",
        "\n",
        "    def BSR_dataframe(bsr: BindingSiteReport) -> pd.DataFrame:\n",
        "        bond_dfs = []\n",
        "        for bond_type in int_bonds:\n",
        "            info_attr = bond_type.lower() + '_info'\n",
        "            features_attr = bond_type.lower() + '_features'\n",
        "            bond_info = getattr(bsr, info_attr, [])\n",
        "            bond_features = getattr(bsr, features_attr, [])\n",
        "            if bond_info and bond_features:\n",
        "                df_bond = pd.DataFrame(bond_info, columns=bond_features)\n",
        "                df_bond['BOND'] = bond_type\n",
        "                bond_dfs.append(df_bond)\n",
        "        if bond_dfs:\n",
        "            final_df = pd.concat(bond_dfs, ignore_index=True)\n",
        "        else:\n",
        "            final_df = pd.DataFrame()\n",
        "        # Force every cell to be cleaned (even if already numeric)\n",
        "        final_df = final_df.applymap(cleanup_cell)\n",
        "        if 'LIGCOO' in final_df.columns:\n",
        "            final_df['LIGCOO'] = final_df['LIGCOO'].apply(clean_coordinate)\n",
        "        if 'PROTCOO' in final_df.columns:\n",
        "            final_df['PROTCOO'] = final_df['PROTCOO'].apply(clean_coordinate)\n",
        "        print(\"\\nDEBUG: Final cleaned DataFrame before writing CSV:\\n\", final_df, \"\\n\")\n",
        "        return final_df\n",
        "\n",
        "    try:\n",
        "        bsr_obj = BSR(inpt_cmpx)\n",
        "        final_df = BSR_dataframe(bsr_obj)\n",
        "        final_df.to_csv(oupt_csv, index=False)\n",
        "        print(\"DEBUG: Interaction CSV saved at:\", oupt_csv)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to generate interaction profile: {e}\")\n",
        "\n",
        "cmpx_file = os.path.join(WRK_DIR, \"cmpx.pdb\")\n",
        "INT_profile = os.path.join(WRK_DIR, \"interpt.csv\")\n",
        "\n",
        "print(\"Generating complex PDB...\")\n",
        "generate_cmpx_pdb(protein_file_reduce, predicted_pose_path_pdb_H, cmpx_file)\n",
        "print(\"Complex PDB generated at:\", cmpx_file)\n",
        "\n",
        "print(\"Running PLIP interaction profiler...\")\n",
        "interaction_profiler(cmpx_file, INT_profile)\n",
        "print(\"Interaction profile saved at:\", INT_profile)\n"
      ],
      "metadata": {
        "id": "0Sp9YtubEsJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Visualising docked pose by LaboDock**  { display-mode: \"form\" }\n",
        "# @markdown **Fix the parameters for visualization** \\\n",
        "\n",
        "# @markdown **PROTEIN MODEL** \\\n",
        "# @markdown Enter the **< Protein >** parameters to be viewed.\n",
        "\n",
        "Protein_type = 'cartoon' # @param ['cartoon', 'cross', 'line', 'sphere', 'stick']\n",
        "Protein_style = 'white' # @param ['chain', 'white', 'whiteCarbon', 'ssJmol', 'ssPyMol', 'b factor', 'hydrophobicity', 'isoelectric points']\n",
        "Protein_opacity = 0.9 # @param {type: 'slider', min:0, max:1, step:0.1}\n",
        "Residue_number = '86, 155, 181, 213, 216, 227, 255, 272, 330 ' # @param {type: 'string'}\n",
        "Surface_type = 'SES' # @param ['VDW', 'SAS', 'SES', 'MS']\n",
        "Surface_style = 'hydrophobicity' # @param ['chain', 'white', 'whiteCarbon', 'ssJmol', 'ssPyMol', 'b factor', 'hydrophobicity', 'isoelectric points']\n",
        "Surface_opacity = 0.7 # @param {type: 'slider', min:0, max:1, step:0.1}\n",
        "Show_protein = True # @param {type: 'boolean'}\n",
        "Show_residue = True # @param {type: 'boolean'}\n",
        "Show_surface = True # @param {type: 'boolean'}\n",
        "Show_gridbox = False # @param {type: 'boolean'}\n",
        "grid_choice  = \"/content/ABHD5/pocket_thr050_Majority0_config_2.txt\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown **LIGAND MODEL** \\\n",
        "# @markdown Enter the **< Ligand >** parameters to be viewed.\n",
        "\n",
        "\n",
        "Docked_ligand_style = 'stick' # @param ['cross', 'line', 'sphere', 'stick']\n",
        "Show_docked_ligand = True # @param {type: 'boolean'}\n",
        "\n",
        "# @markdown **INTERACTION MODEL** \\\n",
        "# @markdown Enter the **< Interaction Type >** to be viewed. \\\n",
        "# @markdown Select or combine from **`HYDROPHOBIC`**, **`HBOND`**, **`WATERBRIDGE`**,\n",
        "# @markdown **`SALTBRIDGE`**, **`PISTACKING`**, **`PICATION`**, **`HALOGEN`**, **`METAL`**. \\\n",
        "# @markdown *Note: All interactions are selected if not provided.*\n",
        "\n",
        "Interaction_type = \"HYDROPHOBIC, HBOND, WATERBRIDGE, SALTBRIDGE, PISTACKING, PICATION, HALOGEN, METAL\" # @param {type: 'string'}\n",
        "Show_interaction = True # @param {type: 'boolean'}\n",
        "\n",
        "# @markdown **OTHER OPTIONS** \\\n",
        "# @markdown Miscellaneous visualisation settings.\n",
        "\n",
        "Slab_view = False # @param {type: 'boolean'}\n",
        "\n",
        "\n",
        "LBSP = LaboSpace(1500, 900)\n",
        "LBSP.load_receptor(protein_file_reduce)\\\n",
        "    .set_style(\n",
        "        show_represent=Show_protein,\n",
        "        represent_type=Protein_type,\n",
        "        represent_style=builtin_style(\n",
        "            style=Protein_style,\n",
        "            opacity=Protein_opacity))\\\n",
        "    .add_residues(\n",
        "        show_residues=Show_residue,\n",
        "        residue_number=Residue_number)\\\n",
        "    .add_surface(\n",
        "        show_surface=Show_surface,\n",
        "        surface_type=Surface_type,\n",
        "        surface_style=builtin_style(\n",
        "            style=Surface_style,\n",
        "            opacity=Surface_opacity))\n",
        "\n",
        "LBSP.load_ligand(predicted_pose_path_pdb_H)\\\n",
        "    .set_style(\n",
        "        show_represent=True,\n",
        "        represent_type=Docked_ligand_style,\n",
        "        represent_style={'colorscheme': 'salmonCarbon'})\n",
        "\n",
        "LBSP.add_interaction(\n",
        "    interaction_file=INT_profile,\n",
        "    show_interaction=Show_interaction,\n",
        "    select_interaction=Interaction_type)\n",
        "\n",
        "cfg_center, cfg_bxsize = extract_config(grid_choice)\n",
        "\n",
        "LBSP.add_gridbox(\n",
        "    show_gridbox=Show_gridbox,\n",
        "    center=cfg_center,\n",
        "    bxsize=cfg_bxsize)\n",
        "\n",
        "LBSP.view_space(\n",
        "    slab_view=Slab_view)\n",
        "\n",
        "show_cscale({Show_protein: Protein_style}, {Show_surface: Surface_style})\n"
      ],
      "metadata": {
        "id": "bwSCjAWMEsOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}